{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "00fef51e",
      "metadata": {
        "id": "00fef51e",
        "outputId": "e0708ac8-c1c2-4da7-df78-21714c5cf5f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Git LFS initialized.\n",
            "Cloning into 'bsc-thesis'...\n",
            "remote: Enumerating objects: 319, done.\u001b[K\n",
            "remote: Counting objects: 100% (63/63), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 319 (delta 28), reused 37 (delta 13), pack-reused 256 (from 1)\u001b[K\n",
            "Receiving objects: 100% (319/319), 10.69 MiB | 15.98 MiB/s, done.\n",
            "Resolving deltas: 100% (175/175), done.\n",
            "/content/bsc-thesis/src\n",
            "\u001b[0m\u001b[01;34mbacktesting\u001b[0m/  \u001b[01;34mdata\u001b[0m/       main.py  \u001b[01;34mpreprocessing\u001b[0m/\n",
            "\u001b[01;34mconfig\u001b[0m/       main.ipynb  \u001b[01;34mmodels\u001b[0m/  \u001b[01;34mutils\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "# if \"preprocessing\" folder in current folders -> cd back to original folder\n",
        "%cd /content\n",
        "import os\n",
        "if os.path.exists(\"bsc-thesis\"):\n",
        "  # if bsc-thesis folder already exists; completely remove\n",
        "  !rm -rf bsc-thesis\n",
        "\n",
        "# this makes sure cached files are readily available (for calling e.g. `gather_data_cached`)\n",
        "!apt-get install git-lfs\n",
        "!git lfs install\n",
        "\n",
        "# cloning repo\n",
        "branch = \"main\"\n",
        "!git clone --branch $branch https://github.com/maviddoerdijk/bsc-thesis.git\n",
        "\n",
        "# moving into project dir\n",
        "%cd bsc-thesis/src\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ta\n",
        "!pip install prophet\n",
        "!pip install pykalman\n",
        "!pip install PyWavelets\n",
        "!pip install curl-cffi"
      ],
      "metadata": {
        "id": "AyuyDvh3gGqT",
        "outputId": "c37b772d-e4de-48bf-f717-0de236ac0b3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AyuyDvh3gGqT",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ta in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ta) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ta) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n",
            "Requirement already satisfied: prophet in /usr/local/lib/python3.11/dist-packages (1.1.6)\n",
            "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from prophet) (1.2.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.11/dist-packages (from prophet) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from prophet) (3.10.0)\n",
            "Requirement already satisfied: pandas>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from prophet) (2.2.2)\n",
            "Requirement already satisfied: holidays<1,>=0.25 in /usr/local/lib/python3.11/dist-packages (from prophet) (0.71)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.11/dist-packages (from prophet) (4.67.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from prophet) (6.5.2)\n",
            "Requirement already satisfied: stanio<2.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from cmdstanpy>=1.0.4->prophet) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from holidays<1,>=0.25->prophet) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.4->prophet) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.4->prophet) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->holidays<1,>=0.25->prophet) (1.17.0)\n",
            "Requirement already satisfied: pykalman in /usr/local/lib/python3.11/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from pykalman) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pykalman) (24.2)\n",
            "Requirement already satisfied: scikit-base<0.13.0 in /usr/local/lib/python3.11/dist-packages (from pykalman) (0.12.2)\n",
            "Requirement already satisfied: scipy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from pykalman) (1.15.2)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from PyWavelets) (2.0.2)\n",
            "Requirement already satisfied: curl-cffi in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from curl-cffi) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.11/dist-packages (from curl-cffi) (2025.4.26)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12.0->curl-cffi) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## specific packages for time moe\n",
        "# !pip install accelerate==0.28.0 # standard google colab version is 1.6.0 (apr 1, 2025), but for stability, we use time moe's 0.28.0 (mar 12, 2024)\n",
        "# !pip install transformers==4.40.1 # standard google colab version is 4.51.3, but time moe repo requirements mention/prefer 4.40.1 for stability\n",
        "# !pip install datasets==2.18.0\n",
        "!pip install flash-attn==2.6.3 # optional but recommended by the repo"
      ],
      "metadata": {
        "id": "ZnSVAB5rydvz",
        "outputId": "71a66d01-7b46-4301-db66-5480f921232f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ZnSVAB5rydvz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn==2.6.3\n",
            "  Downloading flash_attn-2.6.3.tar.gz (2.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/2.6 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.6.3) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.6.3) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.6.3) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.6.3) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.6.3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.6.3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.6.3) (2024.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.6.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.6.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.6.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.6.3) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.6.3) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.6.3) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.6.3) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.6.3) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.6.3) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.6.3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.6.3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.6.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.6.3) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.6.3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.6.3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn==2.6.3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn==2.6.3) (3.0.2)\n",
            "Building wheels for collected packages: flash-attn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Module imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Optional, Callable, Dict, Any\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "import torch.nn as nn\n",
        "import itertools\n",
        "from transformers import AutoModelForCausalLM # contains Time MoE model\n",
        "\n",
        "# Custom Imports\n",
        "from models.statistical_models import create_dataset\n",
        "from preprocessing.cointegration import find_cointegrated_pairs\n",
        "from preprocessing.data_preprocessing import filter_pairs_data\n",
        "from preprocessing.technical_indicators import combine_pairs_data\n",
        "from models.statistical_models import default_normalize\n",
        "from preprocessing.wavelet_denoising import wav_den\n",
        "from preprocessing.filters import step_1_filter_remove_nans, step_2_filter_liquidity\n",
        "from preprocessing.sliding_window import create_sliding_dataset, SlidingWindowDataset\n",
        "\n",
        "## specific caching imports (should be changed in case you want to gather data live)\n",
        "from data.scraper import load_cached_etf_tickers\n",
        "from data.data_collection_cache import gather_data_cached\n",
        "\n",
        "\n",
        "# Any other changes to be made throughout the entire notebook\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "# OPTIONAL: checking whether a specific function is the version you want or not\n",
        "inspect_func = False\n",
        "if inspect_func:\n",
        "  import inspect\n",
        "  print(inspect.getsource(find_cointegrated_pairs)) # in this case, check whether tqdm was actually added"
      ],
      "metadata": {
        "id": "iFVVhPs_gKgR"
      },
      "id": "iFVVhPs_gKgR",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Configs - change these to the desired values to LOAD FROM cache as wanted\n",
        "startDateStr = '2010-10-01'\n",
        "endDateStr = '2024-10-02' # documentation said that endDateStr is exclusive for both yahoofinance and the original code, but actually printing the shapes showed otherwise..\n",
        "instrumentIdsNASDAQandNYSE = load_cached_etf_tickers()\n",
        "data = gather_data_cached(startDateStr, endDateStr, instrumentIdsNASDAQandNYSE, cache_dir='../src/data/cache')\n",
        "data_close_filtered_1, data_open_filtered_1, data_high_filtered_1, data_low_filtered_1, data_vol_filtered_1, data_original_format_filtered_1 = step_1_filter_remove_nans(data['close'], data['open'], data['high'], data['low'], data['vol'], data)\n",
        "data_close_filtered_2, data_open_filtered_2, data_high_filtered_2, data_low_filtered_2, data_vol_filtered_2, data_original_format_filtered_2 = step_2_filter_liquidity(data_close_filtered_1, data_open_filtered_1, data_high_filtered_1, data_low_filtered_1, data_vol_filtered_1, data_original_format_filtered_1)\n",
        "\n",
        "scores, pvalues, pairs = find_cointegrated_pairs(data_original_format_filtered_2)\n",
        "pairs_data = {key:value[1]  for (key, value) in pairs.items()}\n",
        "pairs_data = sorted(pairs_data.items(), key=lambda x: x[1])\n",
        "pairs_data_filtered = filter_pairs_data(pairs_data) # filter based on cointegration in such a way that we can simply pick the highest pair of stocks in the list.\n",
        "# Extract the most highly cointegrated pairs\n",
        "ticker_a, ticker_b = pairs_data_filtered[0][0][0], pairs_data_filtered[0][0][1]\n",
        "pairs_timeseries_df = combine_pairs_data(data_close_filtered_2, data_open_filtered_2, data_high_filtered_2, data_low_filtered_2, data_vol_filtered_2, ticker_a, ticker_b)\n",
        "# Note about pairs_timeseries_df: the timeseries output on which we should train are found in the key \"Spread_Close\"\n",
        "# But, also the input features are the following keys: ['S1_rsi', 'S2_rsi', 'S1_mfi', 'S2_mfi', 'S1_adi', 'S2_adi', 'S1_vpt', 'S2_vpt', 'S1_atr', 'S2_atr', 'S1_bb_ma', 'S2_bb_ma', 'S1_adx', 'S2_adx', 'S1_ema', 'S2_ema', 'S1_macd', 'S2_macd', 'S1_dlr', 'S2_dlr']"
      ],
      "metadata": {
        "id": "tK5jU2WJgcrY",
        "outputId": "c1a3a9df-2cae-434e-cdae-9d4f06e58ffd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "tK5jU2WJgcrY",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 1711 pairs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/bsc-thesis/src/preprocessing/technical_indicators.py:96: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  alpha_c = -sm.OLS(df['S1_close'], df['S2_close']).fit().params[0]\n",
            "/content/bsc-thesis/src/preprocessing/technical_indicators.py:97: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  alpha_o = -sm.OLS(df['S1_open'], df['S2_open']).fit().params[0]\n",
            "/content/bsc-thesis/src/preprocessing/technical_indicators.py:98: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  alpha_h = -sm.OLS(df['S1_high'], df['S2_high']).fit().params[0]\n",
            "/content/bsc-thesis/src/preprocessing/technical_indicators.py:99: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  alpha_l = -sm.OLS(df['S1_low'], df['S2_low']).fit().params[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a bunch of variables based on the existing functions `execute_kalman_workflow` and `execute_transformer_workflow` (Note: Some are changed already)\n",
        "pairs_timeseries: pd.DataFrame = pairs_timeseries_df\n",
        "target_col: str = \"Spread_Close\"\n",
        "burn_in: int = 30 # we remove the first 30 elements, because the largest window used for technical indicators is\n",
        "train_frac: float = 0.90\n",
        "dev_frac: float = 0.05   # remaining part is test\n",
        "look_back: int = 20\n",
        "batch_size: int = 64\n",
        "denoise_fn: Optional[Callable[[pd.Series], np.ndarray]] = wav_den\n",
        "scaler_factory: Callable[..., MinMaxScaler] = MinMaxScaler\n",
        "scaler_kwargs: Optional[Dict[str, Any]] = {\"feature_range\": (0, 1)}\n",
        "normalise_fn: Callable[[pd.Series], pd.Series] = default_normalize\n",
        "delta: float = 1e-3\n",
        "obs_cov_reg: float = 2.\n",
        "trans_cov_avg: float = 0.01\n",
        "obs_cov_avg: float = 1.\n",
        "return_datasets: bool = False\n",
        "verbose: bool = True"
      ],
      "metadata": {
        "id": "qMJPi3nyioFn"
      },
      "id": "qMJPi3nyioFn",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def execute_timemoe_workflow(...):\n",
        "if not target_col in pairs_timeseries.columns:\n",
        "  raise KeyError(f\"pairs_timeseries must contain {target_col}\")\n",
        "\n",
        "# burn the first 30 elements\n",
        "pairs_timeseries_burned = pairs_timeseries.iloc[burn_in:].copy()\n",
        "\n",
        "total_len = len(pairs_timeseries_burned)\n",
        "train_size = int(total_len * train_frac)\n",
        "dev_size   = int(total_len * dev_frac)\n",
        "test_size  = total_len - train_size - dev_size # not used, but for clarity\n",
        "\n",
        "train = pairs_timeseries_burned[:train_size]\n",
        "dev   = pairs_timeseries_burned[train_size:train_size+dev_size] # aka validation\n",
        "test  = pairs_timeseries_burned[train_size+dev_size:]\n",
        "\n",
        "if verbose:\n",
        "    print(f\"Split sizes — train: {len(train)}, dev: {len(dev)}, test: {len(test)}\")\n",
        "\n",
        "if denoise_fn is not None: # denoise using wavelet denoising\n",
        "    train = pd.DataFrame({col: denoise_fn(train[col]) for col in train.columns}) # TODO: unsure whether dev and test should also be denoised?\n",
        "\n",
        "x_scaler = scaler_factory(**scaler_kwargs) # important: the scaler learns parameters, so separate objects must be created for x and y\n",
        "y_scaler = scaler_factory(**scaler_kwargs)\n",
        "\n",
        "# We want a sliding window in our dataset\n",
        "trainX_raw, trainX_scaled, trainY_raw, trainY_scaled = create_sliding_dataset(\n",
        "    train.values, x_scaler=x_scaler, y_scaler=y_scaler, look_back=look_back) # train_X_scaled.shape: (2219, 20, 34) // [(t - look_back), look_back, features]\n",
        "devX_raw,   devX_scaled,   devY_raw,   devY_scaled   = create_sliding_dataset(\n",
        "    dev.values,  x_scaler=x_scaler, y_scaler=y_scaler, look_back=look_back)\n",
        "testX_raw,  testX_scaled,  testY_raw,  testY_scaled  = create_sliding_dataset(\n",
        "    test.values, x_scaler=x_scaler, y_scaler=y_scaler, look_back=look_back)\n",
        "\n",
        "train_ds = SlidingWindowDataset(trainX_scaled, trainY_scaled)\n",
        "dev_ds   = SlidingWindowDataset(devX_scaled, devY_scaled)\n",
        "test_ds  = SlidingWindowDataset(testX_scaled, testY_scaled)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size,\n",
        "                          shuffle=True,  drop_last=True,  num_workers=0)\n",
        "dev_loader   = DataLoader(dev_ds,   batch_size=batch_size,\n",
        "                          shuffle=False, drop_last=False, num_workers=0)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size,\n",
        "                          shuffle=False, drop_last=False, num_workers=0) # extra note: shuffling is turned off for these datasets, because we want to be able to plot over the testing time period\n",
        "\n",
        "# import torch\n",
        "# from transformers import AutoModelForCausalLM\n",
        "if verbose:\n",
        "  print(\"Single tensor shape: {next(iter(train_loader))[0].shape}\")   # torch.Size([64, 20, 34]) //  (batch_size, look_back, features)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    'Maple728/TimeMoE-50M',\n",
        "    device_map=DEVICE,  # use \"cpu\" for CPU inference, and \"cuda\" for GPU inference.\n",
        "    trust_remote_code=True, # interesting name for a\n",
        ")"
      ],
      "metadata": {
        "id": "gQMwa1KhouYZ",
        "outputId": "0277b198-0475-4bf2-bd15-0eab7641d92b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "gQMwa1KhouYZ",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split sizes — train: 3143, dev: 174, test: 176\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}