{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This file is meant for *development* of hyperparameter optimization. Final metrics for hyperparameter optimization can be found at *total_results.ipynb*."
      ],
      "metadata": {
        "id": "qzjibbC1CJzQ"
      },
      "id": "qzjibbC1CJzQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# if \"preprocessing\" folder in current folders -> cd back to original folder\n",
        "%cd /content\n",
        "import os\n",
        "if os.path.exists(\"bsc-thesis\"):\n",
        "  # if bsc-thesis folder already exists; completely remove\n",
        "  !rm -rf bsc-thesis\n",
        "\n",
        "# cloning repo\n",
        "branch = \"main\"\n",
        "!git clone --branch $branch https://github.com/maviddoerdijk/bsc-thesis.git\n",
        "\n",
        "# moving into project dir\n",
        "%cd bsc-thesis/src\n",
        "%ls"
      ],
      "metadata": {
        "id": "oInppSqTDSee"
      },
      "id": "oInppSqTDSee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ta\n",
        "!pip install pykalman\n",
        "!pip install PyWavelets\n",
        "!pip install curl-cffi"
      ],
      "metadata": {
        "id": "CbFlTsKsDeVy"
      },
      "id": "CbFlTsKsDeVy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show scikit-optimize"
      ],
      "metadata": {
        "id": "3E2fuOS-Hha2"
      },
      "id": "3E2fuOS-Hha2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## data gathering imports\n",
        "from utils.helpers import _get_train_dev_frac\n",
        "from preprocessing.filters import step_1_filter_remove_nans, step_2_filter_liquidity\n",
        "from preprocessing.cointegration import find_cointegrated_pairs\n",
        "from preprocessing.data_preprocessing import filter_pairs_data\n",
        "from preprocessing.technical_indicators import combine_pairs_data\n",
        "## specific caching imports (should be changed in case you want to gather data live)\n",
        "from data.scraper import load_cached_etf_tickers\n",
        "from data.data_collection_cache import gather_data_cached, gather_data_cached_using_truncate\n",
        "\n",
        "## workflow imports\n",
        "from models.statistical_models import execute_kalman_workflow\n",
        "\n",
        "## optimize-specific imports\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Real\n",
        "from skopt.utils import use_named_args\n",
        "import numpy as np\n",
        "from typing import Callable, Any, List"
      ],
      "metadata": {
        "id": "3p6ZGiWSH1Bx"
      },
      "id": "3p6ZGiWSH1Bx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Standard Data Gathering Code ###\n",
        "instrumentIdsNASDAQandNYSE = load_cached_etf_tickers()\n",
        "data = gather_data_cached_using_truncate(startDateStr, endDateStr, instrumentIdsNASDAQandNYSE, cache_dir='../src/data/cache')\n",
        "data_close_filtered_1, data_open_filtered_1, data_high_filtered_1, data_low_filtered_1, data_vol_filtered_1, data_original_format_filtered_1 = step_1_filter_remove_nans(data['close'], data['open'], data['high'], data['low'], data['vol'], data)\n",
        "data_close_filtered_2, data_open_filtered_2, data_high_filtered_2, data_low_filtered_2, data_vol_filtered_2, data_original_format_filtered_2 = step_2_filter_liquidity(data_close_filtered_1, data_open_filtered_1, data_high_filtered_1, data_low_filtered_1, data_vol_filtered_1, data_original_format_filtered_1)\n",
        "\n",
        "pairs_data_filtered = gather_pairs_data_cached(startDateStr, endDateStr, instrumentIdsNASDAQandNYSE, cache_dir='../src/data/cache')\n",
        "if pairs_data_filtered is None:\n",
        "  scores, pvalues, pairs = find_cointegrated_pairs(data_original_format_filtered_2)\n",
        "  pairs_data = {key:value[1]  for (key, value) in pairs.items()}\n",
        "  pairs_data = sorted(pairs_data.items(), key=lambda x: x[1])\n",
        "  pairs_data_filtered = filter_pairs_data(pairs_data)\n",
        "\n",
        "ticker_a, ticker_b = pairs_data_filtered[0][0][0], pairs_data_filtered[0][0][1]\n",
        "pairs_timeseries_df = combine_pairs_data(data_close_filtered_2, data_open_filtered_2, data_high_filtered_2, data_low_filtered_2, data_vol_filtered_2, ticker_a, ticker_b)\n",
        "### Standard Data Gathering Code ###"
      ],
      "metadata": {
        "id": "n2zHgzD2H1ka"
      },
      "id": "n2zHgzD2H1ka",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71763103",
      "metadata": {
        "id": "71763103"
      },
      "outputs": [],
      "source": [
        "search_space = [ # 'name' is used directly as a kwarg\n",
        "    Real(1e-5, 1e-3, name='delta', prior='log-uniform'),\n",
        "    Real(0.5, 2.0, name='obs_cov_reg', prior='log-uniform'),\n",
        "    Real(0.01, 0.1, name='trans_cov_avg', prior='log-uniform'),\n",
        "    Real(0.1, 1.0, name='obs_cov_avg', prior='log-uniform')\n",
        "]\n",
        "\n",
        "def bayesian_optimize_workflow(\n",
        "    execute_workflow_fn: Callable,\n",
        "    pairs_data_filtered: List,\n",
        "    data_close_filtered_2: pd.DataFrame,\n",
        "    data_open_filtered_2: pd.DataFrame,\n",
        "    data_high_filtered_2: pd.DataFrame,\n",
        "    data_low_filtered_2: pd.DataFrame,\n",
        "    data_vol_filtered_2: pd.DataFrame,\n",
        "    top_pair_count: int,\n",
        "    start_year: int,\n",
        "    min_end_year: int,\n",
        "    max_end_year: int,\n",
        "    search_space: List[Real]\n",
        ") -> Tuple[\n",
        "    Dict[int, Dict[int, Dict[str, Any]]],  # period_year -> pair_idx -> {\"best_params\":..., \"best_val_mse\":...}\n",
        "    Dict[int, float], # period_year -> average_mse_across_pairs\n",
        "    float # average_mse_across_all_periods\n",
        "    # TODO: also return best_params?\n",
        "]:\n",
        "    param_names = [dim.name for dim in search_space]\n",
        "    all_results = {} # end_year -> pair_idx -> results dict\n",
        "\n",
        "    # time series cross validation: go over all periods\n",
        "    for rolling_end_year in range(min_end_year, max_end_year + 1):\n",
        "        period_results = {}\n",
        "        startDateStr = f\"{start_year}-01-01\"\n",
        "        endDateStr = f\"{rolling_end_year}-12-31\"\n",
        "        startDateStrTest = f\"{rolling_end_year}-01-01\"\n",
        "        endDateStrTest = endDateStr\n",
        "\n",
        "        train_frac, dev_frac = _get_train_dev_frac(startDateStr, endDateStr, startDateStrTest, endDateStrTest)\n",
        "\n",
        "        # for more balanced results: use top x pairs, chosen to be 5 due to stay in within realistic time and resources\n",
        "        for pair_idx in range(top_pair_count):\n",
        "            ticker_a, ticker_b = pairs_data_filtered[pair_idx][0][0], pairs_data_filtered[pair_idx][0][1]\n",
        "            pairs_timeseries_df = combine_pairs_data(data_close_filtered_2, data_open_filtered_2, data_high_filtered_2, data_low_filtered_2, data_vol_filtered_2, ticker_a, ticker_b)\n",
        "\n",
        "            # define the objective for this pair & period (defined here because pairs_timeseries_df is hardcoded and changes based on rolling_end_year and pair_idx)\n",
        "            @use_named_args(search_space)\n",
        "            def objective(**params):\n",
        "                output = execute_workflow_fn(\n",
        "                    pairs_timeseries_df,\n",
        "                    **params,\n",
        "                    verbose=False\n",
        "                )\n",
        "                return output['test_mse']\n",
        "\n",
        "            res = gp_minimize(\n",
        "                func=objective,\n",
        "                dimensions=search_space,\n",
        "                n_calls=30,\n",
        "                n_random_starts=10,\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "            best_params = {k: res.x[i] for i, k in enumerate(param_names)}\n",
        "            best_val_mse = res.fun\n",
        "\n",
        "            period_results[pair_idx] = {\n",
        "              \"best_params\": best_params,\n",
        "              \"best_val_mse\": best_val_mse,\n",
        "              \"period\": rolling_end_year,\n",
        "              \"pair_idx\": pair_idx,\n",
        "              \"pair\": (ticker_a, ticker_b)\n",
        "            }\n",
        "        all_results[rolling_end_year] = period_results\n",
        "    # The end goal of the TSCV is to find the best parameters.\n",
        "    # For each parameter set, average your score over the 6 splits. Pick the parameter set with the best average score.\n",
        "    return all_results"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}