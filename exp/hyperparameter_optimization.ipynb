{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This file is meant for *development* of hyperparameter optimization. Final metrics for hyperparameter optimization can be found at *total_results.ipynb*."
      ],
      "metadata": {
        "id": "qzjibbC1CJzQ"
      },
      "id": "qzjibbC1CJzQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# if \"preprocessing\" folder in current folders -> cd back to original folder\n",
        "%cd /content\n",
        "import os\n",
        "if os.path.exists(\"bsc-thesis\"):\n",
        "  # if bsc-thesis folder already exists; completely remove\n",
        "  !rm -rf bsc-thesis\n",
        "\n",
        "# cloning repo\n",
        "branch = \"main\"\n",
        "!git clone --branch $branch https://github.com/maviddoerdijk/bsc-thesis.git\n",
        "\n",
        "# moving into project dir\n",
        "%cd bsc-thesis/src\n",
        "%ls"
      ],
      "metadata": {
        "id": "oInppSqTDSee"
      },
      "id": "oInppSqTDSee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ta\n",
        "!pip install pykalman\n",
        "!pip install PyWavelets\n",
        "!pip install curl-cffi"
      ],
      "metadata": {
        "id": "CbFlTsKsDeVy"
      },
      "id": "CbFlTsKsDeVy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show scikit-optimize"
      ],
      "metadata": {
        "id": "3E2fuOS-Hha2"
      },
      "id": "3E2fuOS-Hha2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## data gathering imports\n",
        "from utils.helpers import _get_train_dev_frac\n",
        "from preprocessing.filters import step_1_filter_remove_nans, step_2_filter_liquidity\n",
        "from preprocessing.cointegration import find_cointegrated_pairs\n",
        "from preprocessing.data_preprocessing import filter_pairs_data\n",
        "from preprocessing.technical_indicators import combine_pairs_data\n",
        "## specific caching imports (should be changed in case you want to gather data live)\n",
        "from data.scraper import load_cached_etf_tickers\n",
        "from data.data_collection_cache import gather_data_cached, gather_data_cached_using_truncate\n",
        "\n",
        "## workflow imports\n",
        "from models.statistical_models import execute_kalman_workflow\n",
        "\n",
        "## optimize-specific imports\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Real\n",
        "from skopt.utils import use_named_args\n",
        "import numpy as np\n",
        "from typing import Callable, Any, List"
      ],
      "metadata": {
        "id": "3p6ZGiWSH1Bx"
      },
      "id": "3p6ZGiWSH1Bx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Standard Data Gathering Code ###\n",
        "instrumentIdsNASDAQandNYSE = load_cached_etf_tickers()\n",
        "data = gather_data_cached_using_truncate(startDateStr, endDateStr, instrumentIdsNASDAQandNYSE, cache_dir='../src/data/cache')\n",
        "data_close_filtered_1, data_open_filtered_1, data_high_filtered_1, data_low_filtered_1, data_vol_filtered_1, data_original_format_filtered_1 = step_1_filter_remove_nans(data['close'], data['open'], data['high'], data['low'], data['vol'], data)\n",
        "data_close_filtered_2, data_open_filtered_2, data_high_filtered_2, data_low_filtered_2, data_vol_filtered_2, data_original_format_filtered_2 = step_2_filter_liquidity(data_close_filtered_1, data_open_filtered_1, data_high_filtered_1, data_low_filtered_1, data_vol_filtered_1, data_original_format_filtered_1)\n",
        "\n",
        "pairs_data_filtered = gather_pairs_data_cached(startDateStr, endDateStr, instrumentIdsNASDAQandNYSE, cache_dir='../src/data/cache')\n",
        "if pairs_data_filtered is None:\n",
        "  scores, pvalues, pairs = find_cointegrated_pairs(data_original_format_filtered_2)\n",
        "  pairs_data = {key:value[1]  for (key, value) in pairs.items()}\n",
        "  pairs_data = sorted(pairs_data.items(), key=lambda x: x[1])\n",
        "  pairs_data_filtered = filter_pairs_data(pairs_data)\n",
        "\n",
        "ticker_a, ticker_b = pairs_data_filtered[0][0][0], pairs_data_filtered[0][0][1]\n",
        "pairs_timeseries_df = combine_pairs_data(data_close_filtered_2, data_open_filtered_2, data_high_filtered_2, data_low_filtered_2, data_vol_filtered_2, ticker_a, ticker_b)\n",
        "### Standard Data Gathering Code ###"
      ],
      "metadata": {
        "id": "n2zHgzD2H1ka"
      },
      "id": "n2zHgzD2H1ka",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71763103",
      "metadata": {
        "id": "71763103"
      },
      "outputs": [],
      "source": [
        "search_space = [ # 'name' is used directly as a kwarg\n",
        "    Real(1e-5, 1e-3, name='delta', prior='log-uniform'),\n",
        "    Real(0.5, 2.0, name='obs_cov_reg', prior='log-uniform'),\n",
        "    Real(0.01, 0.1, name='trans_cov_avg', prior='log-uniform'),\n",
        "    Real(0.1, 1.0, name='obs_cov_avg', prior='log-uniform')\n",
        "]\n",
        "\n",
        "def bayesian_optimize_workflow(execute_workflow_fn: Callable, pairs_timeseries_df: pd.Series, train_frac: int, dev_frac: int, search_space: List[Real]):\n",
        "    param_names = [dim.name for dim in search_space]\n",
        "\n",
        "    # objective function to minimize\n",
        "    @use_named_args(search_space)\n",
        "    def objective(**params):\n",
        "        output = execute_workflow_fn(\n",
        "            pairs_timeseries_df,\n",
        "            **params,\n",
        "            verbose=False  # for speed\n",
        "        )\n",
        "        val_mse = output['test_mse']\n",
        "        # test MSE gathered from a workflow is the MSE as gathered from the last year.\n",
        "        # In the context inside the worfklow, it is test MSE.\n",
        "        # In the context of our hyperparameter optimization algorithm, it is val MSE.\n",
        "        return val_mse\n",
        "\n",
        "    # run Bayesian optimization\n",
        "    res = gp_minimize(\n",
        "        func=objective,\n",
        "        dimensions=search_space,\n",
        "        n_calls=30, # Number of evaluations of execute_kalman_workflow\n",
        "        n_random_starts=10, # Start with 10 random points before fitting a GP\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # print or save best hyperparameters\n",
        "    best_params = {k: res.x[i] for i, k in enumerate(param_names)}\n",
        "    print(\"Best hyperparameters:\", best_params)\n",
        "    print(\"Best validation MSE:\", res.fun)\n",
        "\n",
        "# time series cross validation: gather metrics over first 6 periods\n",
        "# TODO: average across 5 pairs\n",
        "start_year = 2008\n",
        "min_end_year = 2016\n",
        "max_end_year = 2016 # actually: 2021\n",
        "for rolling_end_year in range(min_end_year, max_end_year + 1):\n",
        "  startDateStr = f\"{start_year}-01-01\"\n",
        "  endDateStr = f\"{rolling_end_year}-12-31\"\n",
        "  startDateStrTest = f\"{end_year}-01-01\"\n",
        "  endDateStrTest = endDateStr\n",
        "  train_frac, dev_frac = _get_train_dev_frac(startDateStr, endDateStr, startDateStrTest, endDateStrTest)\n",
        "  bayesian_optimize_workflow(execute_kalman_workflow, pairs_timeseries_df, train_frac, dev_frac, search_space)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}