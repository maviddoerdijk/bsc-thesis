{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16c98bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# custom code for collecting data\n",
    "from src.data.scraper import load_cached_etf_tickers\n",
    "from src.data.data_collection import gather_data\n",
    "\n",
    "# custom code for collecting cached data\n",
    "from src.data.data_collection_cache import gather_data_cached, save_data, _tickers_to_hash, _get_filename\n",
    "\n",
    "### Configs - change these to the desired values to cache as wanted\n",
    "startDateStr = '2010-10-01'\n",
    "endDateStr = '2024-10-02' # documentation said that endDateStr is exclusive for both yahoofinance and the original code, but actually printing the shapes showed otherwise..\n",
    "instrumentIdsNASDAQandNYSE = load_cached_etf_tickers()\n",
    "###\n",
    "# live_data = gather_data(startDateStr, endDateStr, instrumentIdsNASDAQandNYSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2f14858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'close': Empty DataFrame\n",
       " Columns: [MEDX, DFGX]\n",
       " Index: [],\n",
       " 'open': Empty DataFrame\n",
       " Columns: [MEDX, DFGX]\n",
       " Index: [],\n",
       " 'high': Empty DataFrame\n",
       " Columns: [MEDX, DFGX]\n",
       " Index: [],\n",
       " 'low': Empty DataFrame\n",
       " Columns: [MEDX, DFGX]\n",
       " Index: [],\n",
       " 'vol': Empty DataFrame\n",
       " Columns: [MEDX, DFGX]\n",
       " Index: [],\n",
       " 'yfinance_formatted': Empty DataFrame\n",
       " Columns: [(MEDX, Open), (MEDX, High), (MEDX, Low), (MEDX, Close), (MEDX, Adj Close), (MEDX, Volume), (DFGX, Open), (DFGX, High), (DFGX, Low), (DFGX, Close), (DFGX, Adj Close), (DFGX, Volume)]\n",
       " Index: []}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_data(live_data, startDateStr, endDateStr, instrumentIdsNASDAQandNYSE, cache_dir='../src/data/cache')\n",
    "live_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b893f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'close': Empty DataFrame\n",
       " Columns: [MEDX, DFGX]\n",
       " Index: [],\n",
       " 'open': Empty DataFrame\n",
       " Columns: [MEDX, DFGX]\n",
       " Index: [],\n",
       " 'high': Empty DataFrame\n",
       " Columns: [MEDX, DFGX]\n",
       " Index: [],\n",
       " 'low': Empty DataFrame\n",
       " Columns: [MEDX, DFGX]\n",
       " Index: [],\n",
       " 'vol': Empty DataFrame\n",
       " Columns: [MEDX, DFGX]\n",
       " Index: [],\n",
       " 'yfinance_formatted': Empty DataFrame\n",
       " Columns: [(MEDX, Open), (MEDX, High), (MEDX, Low), (MEDX, Close), (MEDX, Adj Close), (MEDX, Volume), (DFGX, Open), (DFGX, High), (DFGX, Low), (DFGX, Close), (DFGX, Adj Close), (DFGX, Volume)]\n",
       " Index: []}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_data = gather_data_cached(startDateStr, endDateStr, instrumentIdsNASDAQandNYSE, cache_dir='../src/data/cache')\n",
    "assert len(live_data) == len(cached_data), \"The lengths of live and cached data do not match.\"\n",
    "cached_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8218c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = _get_filename(startDateStr, endDateStr, instrumentIdsNASDAQandNYSE)\n",
    "cached_data = gather_data_cached(startDateStr, endDateStr, instrumentIdsNASDAQandNYSE, cache_dir='../src/data/cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "847cb17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc8c42ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pairs: 100%|██████████| 820/820 [02:13<00:00,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 820 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.preprocessing.filters import step_1_filter_remove_nans, step_2_filter_liquidity\n",
    "from src.preprocessing.cointegration import find_cointegrated_pairs\n",
    "from src.preprocessing.data_preprocessing import filter_pairs_data\n",
    "from src.data.data_collection_cache import gather_pairs_data_cached, save_pairs_data_filtered\n",
    "from src.data.scraper import load_cached_etf_tickers\n",
    "from src.data.data_collection_cache import gather_data_cached\n",
    "\n",
    "startDateStr = '2008-10-01'\n",
    "endDateStr = '2018-10-02'\n",
    "instrumentIdsNASDAQandNYSE = load_cached_etf_tickers()\n",
    "data = gather_data_cached(startDateStr, endDateStr, instrumentIdsNASDAQandNYSE, cache_dir='../src/data/cache')\n",
    "data_close_filtered_1, data_open_filtered_1, data_high_filtered_1, data_low_filtered_1, data_vol_filtered_1, data_original_format_filtered_1 = step_1_filter_remove_nans(data['close'], data['open'], data['high'], data['low'], data['vol'], data)\n",
    "data_close_filtered_2, data_open_filtered_2, data_high_filtered_2, data_low_filtered_2, data_vol_filtered_2, data_original_format_filtered_2 = step_2_filter_liquidity(data_close_filtered_1, data_open_filtered_1, data_high_filtered_1, data_low_filtered_1, data_vol_filtered_1, data_original_format_filtered_1)\n",
    "\n",
    "scores, pvalues, pairs = find_cointegrated_pairs(data_original_format_filtered_2) # note, from all 820 pairs, only 95 are returned, because we filter out all pairs that have a cointegration score <0.05\n",
    "pairs_data = {key:value[1]  for (key, value) in pairs.items()}\n",
    "pairs_data = sorted(pairs_data.items(), key=lambda x: x[1])\n",
    "pairs_data_filtered = filter_pairs_data(pairs_data) \n",
    "\n",
    "save_pairs_data_filtered(pairs_data_filtered, startDateStr, endDateStr, instrumentIdsNASDAQandNYSE, cache_dir='../src/data/cache')\n",
    "pairs_data_from_cache = gather_pairs_data_cached(startDateStr, endDateStr, instrumentIdsNASDAQandNYSE, cache_dir='../src/data/cache')\n",
    "pairs_data_from_cache == pairs_data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69959a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pairs: 100%|██████████| 1711/1711 [07:07<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1711 pairs\n",
      "Checking for time period 2010-10-01 2024-10-02, equal is True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pairs: 100%|██████████| 253/253 [01:14<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 253 pairs\n",
      "Checking for time period 2007-01-01 2022-12-31, equal is True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pairs: 100%|██████████| 253/253 [01:26<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 253 pairs\n",
      "Checking for time period 2007-01-01 2024-12-31, equal is True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pairs: 100%|██████████| 210/210 [00:32<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 210 pairs\n",
      "Checking for time period 2007-01-01 2016-12-31, equal is True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pairs: 100%|██████████| 253/253 [01:05<00:00,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 253 pairs\n",
      "Checking for time period 2007-01-01 2021-12-31, equal is True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now do it for all other time periods as well\n",
    "from datetime import datetime\n",
    "def _get_train_dev_frac(startDateStr, endDateStr, startDateStrTest, endDateStrTest, verbose=False):\n",
    "  \"\"\"\n",
    "  For certain periods, we want a specific testing period, and must therefore calculate the train/dev split based on that.\n",
    "  \"\"\"\n",
    "  # convert all 4 dates to datetime\n",
    "  startDateStr = datetime.strptime(startDateStr, '%Y-%m-%d')\n",
    "  endDateStr = datetime.strptime(endDateStr, '%Y-%m-%d')\n",
    "  startDateStrTest = datetime.strptime(startDateStrTest, '%Y-%m-%d')\n",
    "  endDateStrTest = datetime.strptime(endDateStrTest, '%Y-%m-%d')\n",
    "\n",
    "  total_days = (endDateStr - startDateStr).days\n",
    "  test_days = (endDateStrTest - startDateStrTest).days\n",
    "  train_days = total_days - 2 * test_days\n",
    "\n",
    "  train_frac = train_days / total_days\n",
    "  test_frac = test_days / total_days\n",
    "  dev_frac = test_frac\n",
    "\n",
    "  if verbose:\n",
    "    print(f\"train_frac: {train_frac}\")\n",
    "    print(f\"dev_frac: {dev_frac}\")\n",
    "    print(f\"test_frac: {test_frac}\")\n",
    "\n",
    "  return train_frac, dev_frac\n",
    "\n",
    "## PERIOD 1\n",
    "startDateStr1 = '2010-10-01'\n",
    "endDateStr1 = '2024-10-02'\n",
    "train_frac1 = 0.90\n",
    "dev_frac1 = 0.05\n",
    "\n",
    "## PERIOD 2\n",
    "startDateStr2 = '2008-10-01'\n",
    "endDateStr2 = '2018-10-02'\n",
    "train_frac2 = 0.90\n",
    "dev_frac2 = 0.05\n",
    "\n",
    "## PERIOD 3\n",
    "startDateStr3 = '2007-01-01'\n",
    "endDateStr3 = '2022-12-31'\n",
    "# wanted test year: test 2022/01-2022/12\n",
    "startDateStrTest3 = '2022-01-01'\n",
    "endDateStrTest3 = '2022-12-31'\n",
    "train_frac3, dev_frac3 = _get_train_dev_frac(startDateStr3, endDateStr3, startDateStrTest3, endDateStrTest3)\n",
    "\n",
    "## PERIOD 4\n",
    "startDateStr4 = '2007-01-01'\n",
    "endDateStr4 = '2024-12-31'\n",
    "# test 2024/01-2024/12\n",
    "startDateStrTest4 = '2024-01-01'\n",
    "endDateStrTest4 = '2024-12-31'\n",
    "train_frac4, dev_frac4 = _get_train_dev_frac(startDateStr4, endDateStr4, startDateStrTest4, endDateStrTest4)\n",
    "\n",
    "## PERIOD 5\n",
    "startDateStr5 = '2007-01-01'\n",
    "endDateStr5 = '2016-12-31'\n",
    "# test 2014/07-2016\n",
    "startDateStrTest5 = '2014-07-01'\n",
    "endDateStrTest5 = '2016-12-31'\n",
    "train_frac5, dev_frac5 = _get_train_dev_frac(startDateStr5, endDateStr5, startDateStrTest5, endDateStrTest5)\n",
    "\n",
    "## PERIOD 6\n",
    "startDateStr6 = '2007-01-01'\n",
    "endDateStr6 = '2021-12-31'\n",
    "# test 2020/01-2021/12\n",
    "startDateStrTest6 = '2020-01-01'\n",
    "endDateStrTest6 = '2021-12-31'\n",
    "train_frac6, dev_frac6 = _get_train_dev_frac(startDateStr6, endDateStr6, startDateStrTest6, endDateStrTest6)\n",
    "\n",
    "all_inputs = [\n",
    "    (startDateStr1, endDateStr1, train_frac1, dev_frac1),\n",
    "    # (startDateStr2, endDateStr2, train_frac2, dev_frac2),\n",
    "    (startDateStr3, endDateStr3, train_frac3, dev_frac3),\n",
    "    (startDateStr4, endDateStr4, train_frac4, dev_frac4),\n",
    "    (startDateStr5, endDateStr5, train_frac5, dev_frac5),\n",
    "    (startDateStr6, endDateStr6, train_frac6, dev_frac6)\n",
    "]\n",
    "\n",
    "for startDateStr, endDateStr, train_frac, dev_frac in all_inputs:\n",
    "  data = gather_data_cached(startDateStr, endDateStr, instrumentIdsNASDAQandNYSE, cache_dir='../src/data/cache')\n",
    "  data_close_filtered_1, data_open_filtered_1, data_high_filtered_1, data_low_filtered_1, data_vol_filtered_1, data_original_format_filtered_1 = step_1_filter_remove_nans(data['close'], data['open'], data['high'], data['low'], data['vol'], data)\n",
    "  data_close_filtered_2, data_open_filtered_2, data_high_filtered_2, data_low_filtered_2, data_vol_filtered_2, data_original_format_filtered_2 = step_2_filter_liquidity(data_close_filtered_1, data_open_filtered_1, data_high_filtered_1, data_low_filtered_1, data_vol_filtered_1, data_original_format_filtered_1)\n",
    "\n",
    "  scores, pvalues, pairs = find_cointegrated_pairs(data_original_format_filtered_2) # note, from all 820 pairs, only 95 are returned, because we filter out all pairs that have a cointegration score <0.05\n",
    "  pairs_data = {key:value[1]  for (key, value) in pairs.items()}\n",
    "  pairs_data = sorted(pairs_data.items(), key=lambda x: x[1])\n",
    "  pairs_data_filtered = filter_pairs_data(pairs_data) \n",
    "\n",
    "  save_pairs_data_filtered(pairs_data_filtered, startDateStr, endDateStr, instrumentIdsNASDAQandNYSE, cache_dir='../src/data/cache')\n",
    "  pairs_data_from_cache = gather_pairs_data_cached(startDateStr, endDateStr, instrumentIdsNASDAQandNYSE, cache_dir='../src/data/cache')\n",
    "  print(f\"Checking for time period {startDateStr} {endDateStr}, equal is {pairs_data_from_cache == pairs_data_filtered}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
