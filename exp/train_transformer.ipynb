{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6e110d40",
      "metadata": {
        "id": "6e110d40"
      },
      "source": [
        "Using the main.ipynb for training Transformers was too chaotic for me. This file will be ONLY contain code that is absolutely necessary for finding out how to train the Transformer model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if \"preprocessing\" folder in current folders -> cd back to original folder\n",
        "%cd /content\n",
        "import os\n",
        "if os.path.exists(\"bsc-thesis\"):\n",
        "  # if bsc-thesis folder already exists; completely remove\n",
        "  !rm -rf bsc-thesis\n",
        "\n",
        "branch = \"main\"\n",
        "!git clone --branch $branch https://github.com/maviddoerdijk/bsc-thesis.git\n",
        "%cd bsc-thesis/src\n",
        "%ls"
      ],
      "metadata": {
        "id": "Jrv4ZsLMQKsm",
        "outputId": "ef77b59c-228e-4303-df5d-bce5895fcfb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Jrv4ZsLMQKsm",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'bsc-thesis'...\n",
            "remote: Enumerating objects: 508, done.\u001b[K\n",
            "remote: Counting objects: 100% (252/252), done.\u001b[K\n",
            "remote: Compressing objects: 100% (197/197), done.\u001b[K\n",
            "remote: Total 508 (delta 113), reused 153 (delta 46), pack-reused 256 (from 1)\u001b[K\n",
            "Receiving objects: 100% (508/508), 23.30 MiB | 14.42 MiB/s, done.\n",
            "Resolving deltas: 100% (260/260), done.\n",
            "Filtering content: 100% (14/14), 1.75 GiB | 63.81 MiB/s, done.\n",
            "/content/bsc-thesis/src\n",
            "\u001b[0m\u001b[01;34mbacktesting\u001b[0m/  \u001b[01;34mdata\u001b[0m/      main.ipynb  \u001b[01;34mmodels\u001b[0m/         \u001b[01;34mutils\u001b[0m/\n",
            "\u001b[01;34mconfig\u001b[0m/       \u001b[01;34mexternal\u001b[0m/  main.py     \u001b[01;34mpreprocessing\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ta\n",
        "!pip install prophet\n",
        "!pip install pykalman\n",
        "!pip install PyWavelets\n",
        "!pip install curl-cffi"
      ],
      "metadata": {
        "id": "m8qoNvXjSTQR",
        "outputId": "1291d445-9750-4313-b94b-df6e64b361b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "m8qoNvXjSTQR",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ta) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ta) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=5de33609c134a155343d376f9211c66a71d01660a1b590d20c305febf6b090c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/d7/29/7781cc5eb9a3659d032d7d15bdd0f49d07d2b24fec29f44bc4\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n",
            "Requirement already satisfied: prophet in /usr/local/lib/python3.11/dist-packages (1.1.6)\n",
            "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from prophet) (1.2.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.11/dist-packages (from prophet) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from prophet) (3.10.0)\n",
            "Requirement already satisfied: pandas>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from prophet) (2.2.2)\n",
            "Requirement already satisfied: holidays<1,>=0.25 in /usr/local/lib/python3.11/dist-packages (from prophet) (0.72)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.11/dist-packages (from prophet) (4.67.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from prophet) (6.5.2)\n",
            "Requirement already satisfied: stanio<2.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from cmdstanpy>=1.0.4->prophet) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from holidays<1,>=0.25->prophet) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.4->prophet) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.4->prophet) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->holidays<1,>=0.25->prophet) (1.17.0)\n",
            "Collecting pykalman\n",
            "  Downloading pykalman-0.10.1-py2.py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from pykalman) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pykalman) (24.2)\n",
            "Collecting scikit-base<0.13.0 (from pykalman)\n",
            "  Downloading scikit_base-0.12.2-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: scipy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from pykalman) (1.15.3)\n",
            "Downloading pykalman-0.10.1-py2.py3-none-any.whl (248 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.5/248.5 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_base-0.12.2-py3-none-any.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-base, pykalman\n",
            "Successfully installed pykalman-0.10.1 scikit-base-0.12.2\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from PyWavelets) (2.0.2)\n",
            "Requirement already satisfied: curl-cffi in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from curl-cffi) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.11/dist-packages (from curl-cffi) (2025.4.26)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12.0->curl-cffi) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Module imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Optional, Callable, Dict, Any\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "import torch.nn as nn\n",
        "import itertools\n",
        "\n",
        "# Custom Imports\n",
        "from models.statistical_models import create_dataset\n",
        "from data.data_collection import gather_data\n",
        "from data.scraper import load_cached_etf_tickers\n",
        "from preprocessing.cointegration import find_cointegrated_pairs\n",
        "from preprocessing.data_preprocessing import filter_pairs_data\n",
        "from preprocessing.technical_indicators import combine_pairs_data\n",
        "from models.statistical_models import default_normalize\n",
        "from preprocessing.wavelet_denoising import wav_den\n",
        "from preprocessing.filters import step_1_filter_remove_nans, step_2_filter_liquidity\n",
        "from backtesting.trading_strategy import trade\n",
        "from backtesting.utils import calculate_return_uncertainty\n",
        "\n",
        "## caching imports\n",
        "from data.data_collection_cache import gather_data_cached, _get_filename\n",
        "from utils.visualization import plot_return_uncertainty, plot_comparison, plot_train_val_loss\n",
        "\n",
        "\n",
        "# Any other changes to be made throughout the entire notebook\n",
        "plt.style.use('seaborn-v0_8')"
      ],
      "metadata": {
        "id": "9Ybs2x7pSIae"
      },
      "id": "9Ybs2x7pSIae",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: all the functions used here are explained in much more detail in src/main.ipynb, but this notebook is simply focused on finding how to ge the Transformer model to work as I wish.\n",
        "startDateStr = '2008-10-01'\n",
        "endDateStr = '2018-10-02' # documentation said that endDateStr is exclusive for both yahoofinance and the original code, but actually printing the shapes showed otherwise..\n",
        "instrumentIdsNASDAQandNYSE = load_cached_etf_tickers()\n",
        "data = gather_data(startDateStr, endDateStr, instrumentIdsNASDAQandNYSE)\n",
        "data_close_filtered_1, data_open_filtered_1, data_high_filtered_1, data_low_filtered_1, data_vol_filtered_1, data_original_format_filtered_1 = step_1_filter_remove_nans(data['close'], data['open'], data['high'], data['low'], data['vol'], data)\n",
        "data_close_filtered_2, data_open_filtered_2, data_high_filtered_2, data_low_filtered_2, data_vol_filtered_2, data_original_format_filtered_2 = step_2_filter_liquidity(data_close_filtered_1, data_open_filtered_1, data_high_filtered_1, data_low_filtered_1, data_vol_filtered_1, data_original_format_filtered_1)\n",
        "\n",
        "scores, pvalues, pairs = find_cointegrated_pairs(data_original_format_filtered_2)\n",
        "pairs_data = {key:value[1]  for (key, value) in pairs.items()}\n",
        "pairs_data = sorted(pairs_data.items(), key=lambda x: x[1])\n",
        "pairs_data_filtered = filter_pairs_data(pairs_data) # filter based on cointegration in such a way that we can simply pick the highest pair of stocks in the list.\n",
        "# Extract the most highly cointegrated pairs\n",
        "ticker_a, ticker_b = pairs_data_filtered[0][0][0], pairs_data_filtered[0][0][1]\n",
        "pairs_timeseries_df = combine_pairs_data(data_close_filtered_2, data_open_filtered_2, data_high_filtered_2, data_low_filtered_2, data_vol_filtered_2, ticker_a, ticker_b)\n",
        "# Note about pairs_timeseries_df: the timeseries output on which we should train are found in the key \"Spread_Close\"\n",
        "# But, also the input features are the following keys: ['S1_rsi', 'S2_rsi', 'S1_mfi', 'S2_mfi', 'S1_adi', 'S2_adi', 'S1_vpt', 'S2_vpt', 'S1_atr', 'S2_atr', 'S1_bb_ma', 'S2_bb_ma', 'S1_adx', 'S2_adx', 'S1_ema', 'S2_ema', 'S1_macd', 'S2_macd', 'S1_dlr', 'S2_dlr']"
      ],
      "metadata": {
        "id": "lWichRMcQaq9",
        "outputId": "7b4797f5-ce93-4a8a-e93a-1e5470577774",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lWichRMcQaq9",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  777 of 777 completed\n",
            "ERROR:yfinance:\n",
            "494 Failed downloads:\n",
            "ERROR:yfinance:['FINE', 'KPDD', 'INFR', 'UCYB', 'NERD', 'MRAL', 'BSMT', 'IBGL', 'IBTM', 'PLTU', 'BOTT', 'IBTP', 'IWTR', 'LDSF', 'ORCX', 'DLLL', 'DUKX', 'MYCK', 'PYPG', 'HLAL', 'ELFY', 'FEAT', 'AIRL', 'UTWY', 'TXUE', 'BSCX', 'EWJV', 'METD', 'TSYY', 'XFIX', 'DVQQ', 'HYDR', 'AVXC', 'FTGS', 'UMMA', 'TARK', 'QDTY', 'SIXG', 'PCMM', 'IONX', 'QQQS', 'SARK', 'EHLS', 'BEEZ', 'SMCL', 'PDBA', 'TEKX', 'ENDW', 'BTFX', 'MYCL', 'AMZZ', 'PALU', 'RDTY', 'AVGX', 'BSCW', 'GPIX', 'IBTG', 'EVYM', 'UPGR', 'OOQB', 'ABCS', 'PLTD', 'SMST', 'PATN', 'CA', 'PTIR', 'IBTO', 'QRMI', 'BUFM', 'IBTJ', 'GLCR', 'MYCI', 'MCHS', 'QOWZ', 'DAPP', 'PSWD', 'NCIQ', 'DMAT', 'BRRR', 'ERET', 'JMID', 'BULD', 'IBTQ', 'VRTL', 'LEXI', 'MYMI', 'BSMP', 'BEEX', 'SOLT', 'BELT', 'BSMU', 'TDI', 'ALLW', 'PBQQ', 'FDNI', 'CORO', 'FICS', 'ASMG', 'BSCT', 'IBOT', 'TSLQ', 'BABX', 'SUSL', 'WINC', 'TQQY', 'RKLX', 'BAFE', 'NIKL', 'ORR', 'USXF', 'ESMV', 'URNJ', 'TSMU', 'QQQH', 'GLOW', 'EKG', 'MUU', 'USRD', 'OBIL', 'TSMG', 'CALI', 'EGGQ', 'BUG', 'XBIL', 'SPCX', 'CHGX', 'HECO', 'UTHY', 'BSMV', 'TTEQ', 'VGSR', 'RUNN', 'WISE', 'ARVR', 'ZTOP', 'TBIL', 'YOKE', 'BSCU', 'EVMT', 'NEWZ', 'TSLL', 'JPEF', 'TSMX', 'QNXT', 'LGCF', 'FLDB', 'BKWO', 'TSLG', 'CONI', 'IVEG', 'UFO', 'MYCH', 'IBTI', 'NATO', 'BSMQ', 'EMXF', 'SPCY', 'AUMI', 'AMID', 'RAA', 'SPRX', 'MSFL', 'HWAY', 'HERD', 'QQQP', 'BRKD', 'WABF', 'IBBQ', 'PABD', 'GLDY', 'SPAM', 'AVUQ', 'MVLL', 'DTCR', 'ESPO', 'BITS', 'SDTY', 'FDIG', 'CONL', 'LFSC', 'NVDL', 'CCNR', 'QSML', 'UTWO', 'DVAL', 'AMZD', 'BSCV', 'JGLO', 'QQQJ', 'DIVD', 'FBL', 'MSTX', 'IBTH', 'INDH', 'BGRN', 'EBI', 'INRO', 'AMZU', 'QQQM', 'BRHY', 'PMBS', 'MEDX', 'GBUG', 'VBIL', 'TSLS', 'BLCR', 'DUKH', 'FEPI', 'BGRO', 'TCHI', 'NCPB', 'QBIG', 'AGMI', 'AMDG', 'MCSE', 'PQJL', 'ODDS', 'QQQI', 'MYCG', 'GGLL', 'BRNY', 'SOXQ', 'INTW', 'XAIX', 'CEFA', 'MODL', 'AOTG', 'BRKU', 'HIMZ', 'HERO', 'IQQQ', 'MYMG', 'CANC', 'ILIT', 'AVGB', 'JIVE', 'ZIPP', 'ERNZ', 'TSPY', 'QQQT', 'UBND', 'REAI', 'SKRE', 'STNC', 'UTEN', 'QQQY', 'QHDG', 'BKCH', 'TAXE', 'SEIE', 'AOHY', 'NFXS', 'ZAP', 'AVL', 'BTF', 'USSH', 'NVDG', 'IBTF', 'WGMI', 'HOOX', 'ROE', 'TSLR', 'BSCY', 'QCLR', 'OPTZ', 'HEQQ', 'BSMR', 'PQAP', 'ZTWO', 'HCOW', 'AMDL', 'NVDU', 'IHYF', 'SOLZ', 'NVDS', 'DVOL', 'DEMZ', 'QCML', 'JPY', 'SMRI', 'HIDE', 'OZEM', 'DMXF', 'ICOP', 'DWUS', 'TAX', 'QQLV', 'FDIV', 'XCNY', 'BSJV', 'ELIL', 'CLOU', 'PEPS', 'FMUB', 'ALIL', 'BSJS', 'ADBG', 'BKIV', 'IBAT', 'MAXI', 'RAYS', 'GIND', 'APED', 'PSTR', 'CARY', 'AIPI', 'REIT', 'KBAB', 'JEPQ', 'MULL', 'KQQQ', 'FDFF', 'SFLO', 'QQA', 'SKYU', 'CLOD', 'TSMZ', 'WTMY', 'CRWL', 'GFLW', 'DECO', 'ECOW', 'SMCO', 'SMCZ', 'IBGB', 'HOOG', 'FDCF', 'CLSM', 'USDX', 'MQQQ', 'GXDW', 'SLVR', 'MNTL', 'ETEC', 'FDTX', 'CTEC', 'VOLT', 'NZUS', 'FMUN', 'CLOA', 'IBGA', 'MKAM', 'MEMS', 'QQQG', 'ARMG', 'USCL', 'AAPB', 'JDOC', 'BUFC', 'MYCJ', 'BCLO', 'PABU', 'MYMH', 'CRMG', 'BSMS', 'MYMJ', 'WCLD', 'KROP', 'MYCF', 'BSJT', 'CANQ', 'DGCB', 'AQWA', 'BSJW', 'BSJR', 'NIXT', 'COPP', 'XYZG', 'OOSB', 'LGRO', 'GPIQ', 'QBUF', 'PQJA', 'USVN', 'WTBN', 'MBS', 'SMCX', 'WNDY', 'GNOM', 'QTOP', 'NUSB', 'SEIS', 'MSFD', 'USAF', 'IBIT', 'AMDS', 'YSPY', 'DYNI', 'SPAQ', 'DFGX', 'QYLG', 'GSIB', 'UFIV', 'FPXE', 'SOFX', 'GTR', 'ELIS', 'PANG', 'ZTEN', 'THMZ', 'VFLO', 'FMED', 'TDSC', 'HYBI', 'BDGS', 'LAYS', 'IBTL', 'IBGK', 'LDEM', 'EMEQ', 'HFSP', 'WCBR', 'MOOD', 'CPLS', 'WRND', 'CCSB', 'JTEK', 'COWS', 'VCRB', 'VPLS', 'WEEI', 'CZAR', 'QQQA', 'EYEG', 'IONL', 'NPFI', 'WBND', 'QTR', 'TXUG', 'BSVO', 'AMDD', 'RNEW', 'TXSS', 'GGLS', 'TUGN', 'FMTM', 'UYLD', 'WTMU', 'NVDD', 'PQOC', 'AGIX', 'BMAX', 'YQQQ', 'QSIX', 'DFGP', 'AVS', 'BSMW', 'MYCN', 'CHPS', 'IBTK', 'AMUU', 'ZTRE', 'HRTS', 'FCTE', 'SPYQ', 'BTGD', 'TSEL', 'UTRE', 'CAFG', 'TSL', 'RGTX', 'GQQQ', 'UBRL', 'ETHA', 'LITP', 'TMET', 'TUG', 'PPI', 'TEKY', 'VGUS', 'MUD', 'TPLS', 'FBOT', 'EVSD', 'COWG', 'FIVY', 'MCDS', 'MYCM', 'QQJG', 'SMCF', 'BSJU', 'SEEM', 'RDTL', 'COPJ', 'LIVR', 'BRTR', 'AFSC', 'LRND', 'NSI', 'DYTA', 'COIG', 'ABIG', 'BSMY', 'MSFU']: YFPricesMissingError('possibly delisted; no price data found  (1d 2008-10-01 -> 2018-10-02) (Yahoo error = \"Data doesn\\'t exist for startDate = 1222833600, endDate = 1538452800\")')\n",
            "Processing pairs: 100%|██████████| 820/820 [01:43<00:00,  7.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 820 pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_data_filtered"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9O2d3cZRJmT",
        "outputId": "c8478879-e995-4ce1-fe5f-1e22a4f23d81"
      },
      "id": "g9O2d3cZRJmT",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('PFF', 'IGSB'), np.float64(3.0163860840139413e-06)),\n",
              " (('USIG', 'PPH'), np.float64(2.621351855041312e-05)),\n",
              " (('USIG', 'QTEC'), np.float64(5.673295148821592e-05)),\n",
              " (('USIG', 'IFGL'), np.float64(9.623595997163074e-05)),\n",
              " (('IGSB', 'SHY'), np.float64(0.00011659037067853098)),\n",
              " (('IGSB', 'PID'), np.float64(0.00013065834446531597)),\n",
              " (('USIG', 'PHO'), np.float64(0.0002540997299264048)),\n",
              " (('USIG', 'IGF'), np.float64(0.0003383020756817305)),\n",
              " (('USIG', 'ACWI'), np.float64(0.0005322750082700492)),\n",
              " (('IGSB', 'EMB'), np.float64(0.0005964786506590314)),\n",
              " (('IGSB', 'BBH'), np.float64(0.0006129783629473792)),\n",
              " (('SHV', 'BBH'), np.float64(0.0007265501368268785)),\n",
              " (('IFGL', 'IGSB'), np.float64(0.0007860628026761489)),\n",
              " (('SHV', 'PEY'), np.float64(0.0008390644259399934)),\n",
              " (('SHV', 'RTH'), np.float64(0.0009203870168499974)),\n",
              " (('IGSB', 'PEY'), np.float64(0.0009728025177510877)),\n",
              " (('PFF', 'IGIB'), np.float64(0.0010572018262853824)),\n",
              " (('USIG', 'ACWX'), np.float64(0.0013155551359995995)),\n",
              " (('SHV', 'IUSG'), np.float64(0.0013786780658902786)),\n",
              " (('SHV', 'IJT'), np.float64(0.0014508284926312565)),\n",
              " (('SHV', 'PNQI'), np.float64(0.0015495106477397057)),\n",
              " (('SHV', 'PDP'), np.float64(0.0015738718296940393)),\n",
              " (('SHV', 'IUSV'), np.float64(0.0016063982596577924)),\n",
              " (('EMB', 'QTEC'), np.float64(0.0016077806507624126)),\n",
              " (('SHV', 'PKW'), np.float64(0.0016487338852363854)),\n",
              " (('SHV', 'IBB'), np.float64(0.0017164476436032409)),\n",
              " (('SHV', 'SMH'), np.float64(0.0017745151469005657)),\n",
              " (('SHV', 'QTEC'), np.float64(0.00195333634935164)),\n",
              " (('SHV', 'SOXX'), np.float64(0.0020928399322783474)),\n",
              " (('IGIB', 'QTEC'), np.float64(0.0021309913373575313)),\n",
              " (('IGSB', 'IGIB'), np.float64(0.002177118081455445)),\n",
              " (('SHV', 'PRFZ'), np.float64(0.0022853588086710125)),\n",
              " (('USIG', 'EMB'), np.float64(0.0023472480370122723)),\n",
              " (('IGSB', 'SMH'), np.float64(0.0023491164532544353)),\n",
              " (('IGSB', 'ACWX'), np.float64(0.002364277289213397)),\n",
              " (('SHV', 'TUR'), np.float64(0.00248759652845951)),\n",
              " (('USIG', 'PFF'), np.float64(0.0028893237396873475)),\n",
              " (('IGIB', 'ACWX'), np.float64(0.0029442928668766935)),\n",
              " (('IGSB', 'QTEC'), np.float64(0.0032685171810781627)),\n",
              " (('SHV', 'PPH'), np.float64(0.003913262558762661)),\n",
              " (('SHV', 'ACWI'), np.float64(0.0042028041368388095)),\n",
              " (('IGSB', 'PDP'), np.float64(0.0044162351514089645)),\n",
              " (('SHV', 'SCZ'), np.float64(0.004801760999017364)),\n",
              " (('IGSB', 'PRFZ'), np.float64(0.00484131529956771)),\n",
              " (('IGF', 'PKW'), np.float64(0.005158476985929181)),\n",
              " (('IGF', 'PRFZ'), np.float64(0.005412210806923513)),\n",
              " (('IGSB', 'BND'), np.float64(0.0058131777353527475)),\n",
              " (('IFGL', 'IGIB'), np.float64(0.006207489101047659)),\n",
              " (('USIG', 'SHV'), np.float64(0.006464912194162086)),\n",
              " (('USIG', 'PEY'), np.float64(0.006610321058646852)),\n",
              " (('MBB', 'SHY'), np.float64(0.006704854700929743)),\n",
              " (('SHV', 'TLT'), np.float64(0.00709320881362782)),\n",
              " (('USIG', 'BBH'), np.float64(0.007565803345291949)),\n",
              " (('ACWI', 'PRFZ'), np.float64(0.009282485850342956)),\n",
              " (('DVY', 'PRFZ'), np.float64(0.009608508732965844)),\n",
              " (('IGIB', 'PID'), np.float64(0.009700352591466782)),\n",
              " (('IJT', 'IUSG'), np.float64(0.009848999535559438)),\n",
              " (('USIG', 'AAXJ'), np.float64(0.01177151885973243)),\n",
              " (('USIG', 'PID'), np.float64(0.0117845644758897)),\n",
              " (('EMB', 'IGIB'), np.float64(0.013223424933122334)),\n",
              " (('IGSB', 'AAXJ'), np.float64(0.015230230827748444)),\n",
              " (('IGF', 'IUSV'), np.float64(0.016386688170763813)),\n",
              " (('IBB', 'BBH'), np.float64(0.018102709021011812)),\n",
              " (('USIG', 'SHY'), np.float64(0.01987642206448271)),\n",
              " (('SMH', 'QTEC'), np.float64(0.020651345089268507)),\n",
              " (('IGF', 'PDP'), np.float64(0.02269605504731267)),\n",
              " (('PEY', 'PRFZ'), np.float64(0.027028952733878918)),\n",
              " (('USIG', 'IBB'), np.float64(0.027173007630816582)),\n",
              " (('USIG', 'IUSV'), np.float64(0.029761442461994275)),\n",
              " (('PFF', 'EMB'), np.float64(0.03034956100495469)),\n",
              " (('IJT', 'PNQI'), np.float64(0.03053747910256003)),\n",
              " (('USIG', 'TLT'), np.float64(0.031949887557182734)),\n",
              " (('USIG', 'TUR'), np.float64(0.032170709154873446)),\n",
              " (('IGF', 'ACWI'), np.float64(0.03372870953152178)),\n",
              " (('AAXJ', 'ACWX'), np.float64(0.03397177150898138)),\n",
              " (('IUSG', 'PNQI'), np.float64(0.034232810457352755)),\n",
              " (('SHV', 'AAXJ'), np.float64(0.03551332093508156)),\n",
              " (('USIG', 'SOXX'), np.float64(0.0364082326113151)),\n",
              " (('USIG', 'SMH'), np.float64(0.03685174760624103)),\n",
              " (('USIG', 'PIE'), np.float64(0.03771567969517278)),\n",
              " (('SHV', 'ACWX'), np.float64(0.03857246087708073)),\n",
              " (('SHV', 'BND'), np.float64(0.04200630595846145)),\n",
              " (('SHV', 'PFF'), np.float64(0.04326074928518892)),\n",
              " (('USIG', 'PKW'), np.float64(0.04367462464704107)),\n",
              " (('IFGL', 'PFF'), np.float64(0.043748444299460995)),\n",
              " (('SHV', 'IGIB'), np.float64(0.04433602371300183)),\n",
              " (('IFGL', 'EMB'), np.float64(0.04474796898250178)),\n",
              " (('DVY', 'PKW'), np.float64(0.045345405639569084)),\n",
              " (('FEX', 'PRFZ'), np.float64(0.04638094496596566)),\n",
              " (('SHV', 'IGSB'), np.float64(0.046669857903621)),\n",
              " (('SHV', 'PID'), np.float64(0.04912722005192086)),\n",
              " (('SHV', 'EMB'), np.float64(0.04918114619598729)),\n",
              " (('IEI', 'TLT'), np.float64(0.049191601403935754)),\n",
              " (('PKW', 'PEY'), np.float64(0.049361446863501746)),\n",
              " (('SHV', 'PIE'), np.float64(0.0499932704426262))]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save variable `data` somewhere\n",
        "# import pickle\n",
        "# with open('data_2010_10_01_2024_10_02_142_failed.pkl', 'wb') as f:\n",
        "#     pickle.dump(data, f)\n",
        "pick_pairs_data_position = 0\n",
        "ticker_a, ticker_b = pairs_data_filtered[pick_pairs_data_position][0][0], pairs_data_filtered[pick_pairs_data_position][0][1]\n",
        "pairs_timeseries_df = combine_pairs_data(data_close_filtered_2, data_open_filtered_2, data_high_filtered_2, data_low_filtered_2, data_vol_filtered_2, ticker_a, ticker_b)"
      ],
      "metadata": {
        "id": "czLMTq_pX0G0"
      },
      "id": "czLMTq_pX0G0",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a bunch of variables based on the existing function `execute_kalman_workflow` (Note: Some are changed already)\n",
        "pairs_timeseries: pd.DataFrame = pairs_timeseries_df\n",
        "target_col: str = \"Spread_Close\"\n",
        "burn_in: int = 30 # we remove the first 30 elements, because the largest window used for technical indicators is\n",
        "train_frac: float = 0.90\n",
        "dev_frac: float = 0.05   # remaining part is test\n",
        "look_back: int = 20\n",
        "batch_size: int = 64\n",
        "denoise_fn: Optional[Callable[[pd.Series], np.ndarray]] = wav_den\n",
        "scaler_factory: Callable[..., MinMaxScaler] = MinMaxScaler\n",
        "scaler_kwargs: Optional[Dict[str, Any]] = {\"feature_range\": (0, 1)}\n",
        "normalise_fn: Callable[[pd.Series], pd.Series] = default_normalize\n",
        "delta: float = 1e-3\n",
        "obs_cov_reg: float = 2.\n",
        "trans_cov_avg: float = 0.01\n",
        "obs_cov_avg: float = 1.\n",
        "return_datasets: bool = False\n",
        "verbose: bool = False"
      ],
      "metadata": {
        "id": "5Zq0luAneeoD"
      },
      "id": "5Zq0luAneeoD",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def execute_transformer_workflow(\n",
        "  pairs_timeseries: pd.DataFrame,\n",
        "  target_col: str = \"Spread_Close\",\n",
        "  burn_in: int = 30, # we remove the first 30 elements, because the largest window used for technical indicators is\n",
        "  train_frac: float = 0.90,\n",
        "  dev_frac: float = 0.05,   # remaining part is test\n",
        "  look_back: int = 20,\n",
        "  batch_size: int = 64,\n",
        "  epochs: int = 400,\n",
        "  patience: int = 150,\n",
        "  denoise_fn: Optional[Callable[[pd.Series], np.ndarray]] = wav_den,\n",
        "  scaler_factory: Callable[..., MinMaxScaler] = MinMaxScaler,\n",
        "  scaler_kwargs: Optional[Dict[str, Any]] = {\"feature_range\": (0, 1)},\n",
        "  normalise_fn: Callable[[pd.Series], pd.Series] = default_normalize,\n",
        "  delta: float = 1e-3,\n",
        "  obs_cov_reg: float = 2.,\n",
        "  trans_cov_avg: float = 0.01,\n",
        "  obs_cov_avg: float = 1.,\n",
        "  return_datasets: bool = False,\n",
        "  verbose: bool = False,\n",
        "  add_technical_indicators: bool = True,\n",
        "  result_parent_dir: str = \"data/results\",\n",
        "  filename_base: str = \"data_begindate_enddate_hash.pkl\",\n",
        "  pair_tup_str: str = \"(?,?)\" # Used for showing which tuple was used in plots, example: \"(QQQ, SPY)\"\n",
        "):\n",
        "  if not target_col in pairs_timeseries.columns:\n",
        "    raise KeyError(f\"pairs_timeseries must contain {target_col}\")\n",
        "\n",
        "  # burn the first 30 elements\n",
        "  pairs_timeseries_burned = pairs_timeseries.iloc[burn_in:].copy()\n",
        "\n",
        "  total_len = len(pairs_timeseries_burned)\n",
        "  train_size = int(total_len * train_frac)\n",
        "  dev_size   = int(total_len * dev_frac)\n",
        "  test_size  = total_len - train_size - dev_size # not used, but for clarity\n",
        "\n",
        "  train = pairs_timeseries_burned[:train_size]\n",
        "  dev   = pairs_timeseries_burned[train_size:train_size+dev_size] # aka validation\n",
        "  test  = pairs_timeseries_burned[train_size+dev_size:]\n",
        "\n",
        "  train_multivariate = train.copy()\n",
        "  dev_multivariate   = dev.copy() # only for completeness\n",
        "  test_multivariate  = test.copy() # only for completeness\n",
        "\n",
        "  if verbose:\n",
        "      print(f\"Split sizes — train: {len(train)}, dev: {len(dev)}, test: {len(test)}\")\n",
        "\n",
        "  if denoise_fn is not None: # denoise using wavelet denoising\n",
        "      train = pd.DataFrame({col: denoise_fn(train[col]) for col in train.columns}) # TODO: unsure whether dev and test should also be denoised?\n",
        "\n",
        "  x_scaler = scaler_factory(**scaler_kwargs) # important: the scaler learns parameters, so separate objects must be created for x and y\n",
        "  y_scaler = scaler_factory(**scaler_kwargs)\n",
        "\n",
        "  if not add_technical_indicators:\n",
        "      train = train[[target_col]]\n",
        "      dev = dev[[target_col]]\n",
        "      test = test[[target_col]]\n",
        "\n",
        "  # We want a sliding window in our dataset\n",
        "  # TODO: defining this function should not be part of workflow, but imported from a custom module\n",
        "  def create_sliding_dataset(mat: np.ndarray,\n",
        "                            x_scaler: MinMaxScaler,\n",
        "                            y_scaler: MinMaxScaler,\n",
        "                            look_back: int = 20):\n",
        "      \"\"\"\n",
        "      X  -> (samples, look_back, features)\n",
        "      y  -> (samples, 1)   — the next-step Spread_Close (just 1 day in advance)\n",
        "      \"\"\"\n",
        "      X, y = [], []\n",
        "      for i in range(len(mat) - look_back):\n",
        "          X.append(mat[i : i + look_back, :]) # window\n",
        "          y.append(mat[i + look_back, 0]) # value right after the window\n",
        "      X, y = np.array(X), np.array(y).reshape(-1, 1)\n",
        "\n",
        "      # scale per feature (fit on the training set once!)\n",
        "      X_scaled = x_scaler.fit_transform(\n",
        "          X.reshape(-1, X.shape[-1])\n",
        "      ).reshape(X.shape)\n",
        "      y_scaled = y_scaler.fit_transform(y)\n",
        "\n",
        "      return X, X_scaled, y, y_scaled\n",
        "\n",
        "  trainX_raw, trainX_scaled, trainY_raw, trainY_scaled = create_sliding_dataset(\n",
        "      train.values, x_scaler=x_scaler, y_scaler=y_scaler, look_back=look_back) # train_X_scaled.shape: (2219, 20, 34) // [(t - look_back), look_back, features]\n",
        "  devX_raw,   devX_scaled,   devY_raw,   devY_scaled   = create_sliding_dataset(\n",
        "      dev.values,  x_scaler=x_scaler, y_scaler=y_scaler, look_back=look_back)\n",
        "  testX_raw,  testX_scaled,  testY_raw,  testY_scaled  = create_sliding_dataset(\n",
        "      test.values, x_scaler=x_scaler, y_scaler=y_scaler, look_back=look_back)\n",
        "\n",
        "\n",
        "  # use pytorch Dataset class\n",
        "  class SlidingWindowDataset(Dataset):\n",
        "      def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "          #  cast to float32 once to avoid repeated conversions\n",
        "          self.X = torch.tensor(X, dtype=torch.float32)      # (N, L, F)\n",
        "          self.y = torch.tensor(y, dtype=torch.float32)      # (N, 1)\n",
        "\n",
        "      def __len__(self):\n",
        "          return self.X.shape[0]\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          return self.X[idx], self.y[idx]                    # each X: (L, F)\n",
        "\n",
        "  train_ds = SlidingWindowDataset(trainX_scaled, trainY_scaled)\n",
        "  dev_ds   = SlidingWindowDataset(devX_scaled, devY_scaled)\n",
        "  test_ds  = SlidingWindowDataset(testX_scaled, testY_scaled)\n",
        "\n",
        "  train_loader = DataLoader(train_ds, batch_size=batch_size,\n",
        "                            shuffle=True,  drop_last=True,  num_workers=0)\n",
        "  dev_loader   = DataLoader(dev_ds,   batch_size=batch_size,\n",
        "                            shuffle=False, drop_last=False, num_workers=0)\n",
        "  test_loader  = DataLoader(test_ds,  batch_size=batch_size,\n",
        "                            shuffle=False, drop_last=False, num_workers=0)\n",
        "\n",
        "  print(next(iter(train_loader))[0].shape)   # torch.Size([64, 20, 34]) //  (batch_size, look_back, features)\n",
        "\n",
        "  class TimeSeriesTransformerv1(nn.Module):\n",
        "    \"\"\"\n",
        "    This version (v1) uses:\n",
        "    * learnable positional embeddings (simple, so no RoPE and no sinusoidal)\n",
        "    * only an encoder (followed by a regression head that transforms from form (seq_len, d_model) into (1), with the output form being the Spread_Close prediction)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_features: int,\n",
        "        seq_len: int,\n",
        "        d_model: int = 128,\n",
        "        nhead: int = 8,\n",
        "        num_layers: int = 4,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # token projection (linear layer)\n",
        "        self.input_proj = nn.Linear(n_features, d_model)\n",
        "\n",
        "        # learnable positional embedding  (1, seq_len, d_model)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, seq_len, d_model))\n",
        "\n",
        "        # encoder (important part)\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=d_model * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True, # keeps (batch, seq, dim)\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers)\n",
        "\n",
        "        # regression head (mainly helps in getting to the right output format)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(start_dim=1), # (batch, seq_len*d_model)\n",
        "            nn.Linear(seq_len * d_model, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # x: (batch, seq_len, n_features)\n",
        "        x = self.input_proj(x) + self.pos_emb\n",
        "        x = self.encoder(x) # (batch, seq_len, d_model)\n",
        "        return self.head(x) # (batch, 1)\n",
        "\n",
        "  n_features = trainX_scaled.shape[-1]\n",
        "\n",
        "  DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  model  = TimeSeriesTransformerv1(\n",
        "              n_features=n_features,\n",
        "              seq_len=look_back,\n",
        "              d_model=128,\n",
        "              nhead=8,\n",
        "              num_layers=4,\n",
        "              dropout=0.1).to(DEVICE)\n",
        "\n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n",
        "\n",
        "  EPOCHS = epochs\n",
        "  PATIENCE = patience\n",
        "\n",
        "  # implement the early stopping logic manually\n",
        "  best_val = float(\"inf\")\n",
        "  epochs_no_improve = 0\n",
        "  print_per_n = 10\n",
        "\n",
        "  # save train_loss and val_loss to lists for plotting\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  for epoch in range(1, EPOCHS + 1):\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      for x_batch, y_batch in train_loader:\n",
        "          x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
        "          optimizer.zero_grad()\n",
        "          preds = model(x_batch).squeeze(-1)\n",
        "          loss  = criterion(preds, y_batch.squeeze(-1))\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          running_loss += loss.item() * x_batch.size(0)\n",
        "      train_loss = running_loss / len(train_loader.dataset) # epoch loss = running loss / N samples\n",
        "      train_losses.append(train_loss)\n",
        "\n",
        "      model.eval()\n",
        "      running_loss_val = 0.0\n",
        "      with torch.no_grad():\n",
        "          for x_batch, y_batch in dev_loader:\n",
        "              x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
        "              preds  = model(x_batch).squeeze(-1)\n",
        "              running_loss_val += criterion(preds, y_batch.squeeze(-1)).item() * x_batch.size(0)\n",
        "      val_loss = running_loss_val / len(dev_loader.dataset) # again, epoch loss = running loss / N samples\n",
        "      val_losses.append(val_loss)\n",
        "\n",
        "      # print losses in a pretty way\n",
        "      if epoch % print_per_n == 0:\n",
        "        print(f\"Epoch {epoch:03d} | train MSE {train_loss:.6f} | val MSE {val_loss:.6f}\")\n",
        "\n",
        "\n",
        "      # manual early stopping logic\n",
        "      if val_loss < best_val - 1e-5: # 1e-5 to not actually make it zero\n",
        "          best_val = val_loss\n",
        "          epochs_no_improve = 0\n",
        "          torch.save(model.state_dict(), \"best_transformer.pt\")\n",
        "      else:\n",
        "          epochs_no_improve += 1\n",
        "          if epochs_no_improve >= PATIENCE:\n",
        "              print(\"Early stopping triggered.\")\n",
        "              break\n",
        "\n",
        "  # Now, let's run the model on the testset\n",
        "  # made sure we're in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  all_preds, all_targets = [], []\n",
        "  with torch.no_grad(): # note for myself: torch.no_grad() makes sure that individual weights are not stored in memory, because we would only need to know those during learning, not during inference\n",
        "      for x_test_batch, y_test_batch in test_loader:\n",
        "        x_test_batch = x_test_batch.to(DEVICE)\n",
        "        preds = model(x_test_batch) # make predictions using model\n",
        "        # transform the preds and targets back to numpy, as these need to be inverted with the scaler, which expects numpy not tensors\n",
        "        preds = preds.cpu().numpy()\n",
        "        y_test_batch = y_test_batch.cpu().numpy()\n",
        "        all_preds.append(preds)\n",
        "        all_targets.append(y_test_batch)\n",
        "\n",
        "  # maybe too much explanation here, but y_hat and y_true respectively represent the predicted and ground truth values\n",
        "  y_hat_scaled = np.concatenate(all_preds).reshape(-1, 1)\n",
        "  y_true_scaled = np.concatenate(all_targets).reshape(-1, 1)\n",
        "\n",
        "  y_hat = y_scaler.inverse_transform(y_hat_scaled)\n",
        "  y_true = y_scaler.inverse_transform(y_true_scaled)\n",
        "\n",
        "  test_mse = np.mean((y_hat - y_true) ** 2)\n",
        "  print(f\"Test MSE  : {test_mse:.6f}\")\n",
        "\n",
        "  ## Trading\n",
        "  test_s1_shortened = test_multivariate['S1_close'].iloc[look_back:]\n",
        "  test_s2_shortened = test_multivariate['S2_close'].iloc[look_back:] # use multivariate versions, so we can still access cols like 'S1_close' and 'S2_close'\n",
        "  test_index_shortened = test_multivariate.index[look_back:] # officially doesn't really matter whether to use `test_multivariate` or `test`, but do it like this for consistency\n",
        "  forecast_test_shortened_series = pd.Series(y_hat.squeeze(), index=test_index_shortened)\n",
        "  gt_test_shortened_series = pd.Series(y_true.squeeze(), index=test_index_shortened)\n",
        "\n",
        "  spread_gt_series = pd.Series(y_true.squeeze(), index=test_index_shortened)\n",
        "  gt_returns = trade(\n",
        "      S1 = test_s1_shortened,\n",
        "      S2 = test_s2_shortened,\n",
        "      spread = spread_gt_series,\n",
        "      window_long = 30,\n",
        "      window_short = 5,\n",
        "      position_threshold = 1.0,\n",
        "      clearing_threshold = 0.5\n",
        "  )\n",
        "  gt_yoy = ((gt_returns[-1] / gt_returns[0])**(365 / len(gt_returns)) - 1)\n",
        "\n",
        "  ## Trading: Mean YoY\n",
        "  min_position = 2.00\n",
        "  max_position = 4.00\n",
        "  min_clearing = 0.30\n",
        "  max_clearing = 0.70\n",
        "  position_thresholds = np.linspace(min_position, max_position, num=10)\n",
        "  clearing_thresholds = np.linspace(min_clearing, max_clearing, num=10)\n",
        "  yoy_mean, yoy_std = calculate_return_uncertainty(test_s1_shortened, test_s2_shortened, forecast_test_shortened_series, position_thresholds=position_thresholds, clearing_thresholds=clearing_thresholds)\n",
        "\n",
        "\n",
        "  ## The variables that should be returned, according to what was returned by the `execute_kalman_workflow` func:\n",
        "  # give same output as was originally the case\n",
        "  if add_technical_indicators:\n",
        "    current_result_dir = filename_base.replace(\".pkl\", \"_transformer\")\n",
        "  else:\n",
        "    current_result_dir = filename_base.replace(\".pkl\", \"_transformer_without_ta\")\n",
        "  result_dir = os.path.join(result_parent_dir, current_result_dir)\n",
        "  if not os.path.exists(result_dir):\n",
        "      os.makedirs(result_dir)\n",
        "\n",
        "  ### Plotting #####\n",
        "  # 1. Train val loss\n",
        "  train_val_loss_filename = plot_train_val_loss(train_losses, val_losses, workflow_type=\"Transformer\", pair_tup_str=pair_tup_str, result_dir=result_dir, verbose=verbose, filename_base=filename_base)\n",
        "\n",
        "  # 2. yoy returns\n",
        "  yoy_returns_filename = plot_return_uncertainty(test_s1_shortened, test_s2_shortened, forecast_test_shortened_series, test_index_shortened, look_back, position_thresholds=position_thresholds, clearing_thresholds=clearing_thresholds, verbose=verbose, result_dir=result_dir, filename_base=filename_base)\n",
        "\n",
        "  # 3. predicted vs actual spread plot\n",
        "  predicted_vs_actual_spread_filename = plot_comparison(gt_test_shortened_series, forecast_test_shortened_series, test_index_shortened, workflow_type=\"Kalman Filter\", pair_tup_str=pair_tup_str, verbose=verbose, result_dir=result_dir, filename_base=filename_base)\n",
        "\n",
        "  ### Plotting #####\n",
        "  plot_filenames = {\n",
        "      \"yoy_returns\": yoy_returns_filename,\n",
        "      \"predicted_vs_actual_spread\": predicted_vs_actual_spread_filename,\n",
        "      \"train_val_loss\": train_val_loss_filename\n",
        "  }\n",
        "  output: Dict[str, Any] = dict(\n",
        "      val_mse=val_losses[-1],\n",
        "      test_mse=test_mse,\n",
        "      yoy_mean=yoy_mean,\n",
        "      yoy_std=yoy_std,\n",
        "      gt_yoy=gt_yoy,\n",
        "      result_parent_dir=result_parent_dir,\n",
        "      plot_filenames=plot_filenames\n",
        "  )\n",
        "\n",
        "  results_str = f\"\"\"\n",
        "Validation MSE: {output['val_mse']}\n",
        "Test MSE: {output['test_mse']}\n",
        "YOY Returns: {output['yoy_mean'] * 100:.2f}%\n",
        "YOY Std: +- {output['yoy_std'] * 100:.2f}%\n",
        "GT Yoy: {output['gt_yoy'] * 100:.2f}%\n",
        "Plot filepath parent dir: {output['result_parent_dir']}\n",
        "Plot filenames: {output['plot_filenames']}\n",
        "  \"\"\"\n",
        "  with open(os.path.join(result_dir, \"results.txt\"), \"w\") as f:\n",
        "      f.write(results_str)\n",
        "  if verbose:\n",
        "    print(results_str)\n",
        "\n",
        "  if return_datasets:\n",
        "      output.update(\n",
        "          dict(train=train, dev=dev, test=test,\n",
        "                datasets=dict(\n",
        "                    train=(trainX_raw, trainX_scaled, trainY_raw, trainY_scaled),\n",
        "                    dev  =(devX_raw,   devX_scaled,   devY_raw,   devY_scaled),\n",
        "                    test =(testX_raw,  testX_scaled,  testY_raw,  testY_scaled)\n",
        "                ))\n",
        "\n",
        "      )\n",
        "  return output\n",
        "\n",
        "output = execute_transformer_workflow(pairs_timeseries_df, verbose=True, result_parent_dir=\"data/results\", filename_base=_get_filename(startDateStr, endDateStr, instrumentIdsNASDAQandNYSE), pair_tup_str=f\"({ticker_a},{ticker_b})\", epochs=20)\n",
        "output_without_tas = execute_transformer_workflow(pairs_timeseries_df, verbose=True, add_technical_indicators=False, result_parent_dir=\"data/results\", filename_base=_get_filename(startDateStr, endDateStr, instrumentIdsNASDAQandNYSE), pair_tup_str=f\"({ticker_a},{ticker_b})\", epochs=20)"
      ],
      "metadata": {
        "id": "7iJKFnTtfZ_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1108cc43-e928-4c50-f4fd-327ff2d8ad66"
      },
      "id": "7iJKFnTtfZ_l",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split sizes — train: 2239, dev: 124, test: 125\n",
            "torch.Size([64, 20, 34])\n",
            "Epoch 010 | train MSE 0.007178 | val MSE 0.027906\n",
            "Epoch 020 | train MSE 0.006499 | val MSE 0.024379\n",
            "Test MSE  : 0.058034\n",
            "Saved plot to data/results/data_2008_10_01_2018_10_02_4416cb3b_transformer/data_2008_10_01_2018_10_02_4416cb3b_train_val_loss.png\n",
            "Saved plot to data/results/data_2008_10_01_2018_10_02_4416cb3b_transformer/data_2008_10_01_2018_10_02_4416cb3b_plot_thresholds.png\n",
            "Saved plot to data/results/data_2008_10_01_2018_10_02_4416cb3b_transformer/data_2008_10_01_2018_10_02_4416cb3b_groundtruth_comparison.png\n",
            "\n",
            "Validation MSE: 0.024378877133131027\n",
            "Test MSE: 0.05803408473730087\n",
            "YOY Returns: 8.13%\n",
            "YOY Std: +- 0.40%\n",
            "GT Yoy: -7.44%\n",
            "Plot filepath parent dir: data/results\n",
            "Plot filenames: {'yoy_returns': 'data_2008_10_01_2018_10_02_4416cb3b_plot_thresholds.png', 'predicted_vs_actual_spread': 'data_2008_10_01_2018_10_02_4416cb3b_groundtruth_comparison.png', 'train_val_loss': 'data_2008_10_01_2018_10_02_4416cb3b_train_val_loss.png'}\n",
            "  \n",
            "Split sizes — train: 2239, dev: 124, test: 125\n",
            "torch.Size([64, 20, 1])\n",
            "Epoch 010 | train MSE 0.006726 | val MSE 0.039263\n",
            "Epoch 020 | train MSE 0.005829 | val MSE 0.022520\n",
            "Test MSE  : 0.045982\n",
            "Saved plot to data/results/data_2008_10_01_2018_10_02_4416cb3b_transformer_without_ta/data_2008_10_01_2018_10_02_4416cb3b_train_val_loss.png\n",
            "Saved plot to data/results/data_2008_10_01_2018_10_02_4416cb3b_transformer_without_ta/data_2008_10_01_2018_10_02_4416cb3b_plot_thresholds.png\n",
            "Saved plot to data/results/data_2008_10_01_2018_10_02_4416cb3b_transformer_without_ta/data_2008_10_01_2018_10_02_4416cb3b_groundtruth_comparison.png\n",
            "\n",
            "Validation MSE: 0.02252034040597769\n",
            "Test MSE: 0.04598179832100868\n",
            "YOY Returns: 0.47%\n",
            "YOY Std: +- 0.04%\n",
            "GT Yoy: -0.36%\n",
            "Plot filepath parent dir: data/results\n",
            "Plot filenames: {'yoy_returns': 'data_2008_10_01_2018_10_02_4416cb3b_plot_thresholds.png', 'predicted_vs_actual_spread': 'data_2008_10_01_2018_10_02_4416cb3b_groundtruth_comparison.png', 'train_val_loss': 'data_2008_10_01_2018_10_02_4416cb3b_train_val_loss.png'}\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def trade(\n",
        "    S1: pd.Series,\n",
        "    S2: pd.Series,\n",
        "    spread: pd.Series, # model-predicted spread, used for the strategy\n",
        "    window_long: int, # long time-span moving-average window (and window used for stdev) (e.g. 60)\n",
        "    window_short: int, # short time span moving-average window (e.g. 5)\n",
        "    position_threshold: float = 1.0,\n",
        "    clearing_threshold: float = 0.5\n",
        ") -> float: # simulated profit-and-loss\n",
        "    ma_long = spread.rolling(window=window_long, center=False).mean()\n",
        "    ma_short = spread.rolling(window=window_short, center=False).mean()\n",
        "    std = spread.rolling(window=window_short, center=False).std()\n",
        "    zscore = (ma_long - ma_short)/std\n",
        "\n",
        "    cash, qty_s1, qty_s2 = 0.0, 0, 0\n",
        "    returns = []\n",
        "\n",
        "    for i in range(len(spread)):\n",
        "      # go through zscore for each timestep\n",
        "      if zscore.iloc[i] > position_threshold: # option 1: sell short\n",
        "        cash += S1.iloc[i] - S2.iloc[i] * spread.iloc[i]\n",
        "        qty_s1 -= 1\n",
        "        qty_s2 += spread.iloc[i] # following the classic pairs-trade: this buys β shares in the case that spread_t = S1_t + β * S2_t\n",
        "      elif zscore.iloc[i] < -position_threshold: # option 2: buy long\n",
        "        cash -= S1.iloc[i] - S2.iloc[i] * spread.iloc[i]\n",
        "        qty_s1 += 1\n",
        "        qty_s2 -= spread.iloc[i] # same as bove, but other way around\n",
        "      elif abs(zscore.iloc[i]) < clearing_threshold: # option 3: go neutral, clearing all positions\n",
        "        cash += qty_s1 * S1.iloc[i] - qty_s2 * S2.iloc[i] # Closing on S1 is gained (we make a profit if we have a positive amount of S1, which we sell), closing on S2 is lost (we need to buy back the shorted positions, losing cash)\n",
        "        qty_s1, qty_s2 = 0, 0\n",
        "      returns.append(cash)\n",
        "    return returns\n",
        "\n",
        "spread_pred_series = pd.Series(y_hat.squeeze(), index=test.index[look_back:])  # align lengths\n",
        "spread_gt_series = pd.Series(y_true.squeeze(), index=test.index[look_back:])\n",
        "returns = trade(\n",
        "    S1 = test['S1_close'].iloc[look_back:],\n",
        "    S2 = test['S2_close'].iloc[look_back:],\n",
        "    spread = spread_pred_series,\n",
        "    window_long = 30,\n",
        "    window_short = 5,\n",
        "    position_threshold = 1.0,\n",
        "    clearing_threshold = 0.5\n",
        ")\n",
        "print(\"Simulated P&L:\", returns[-1])\n",
        "\n",
        "# plot returns\n",
        "plt.plot(returns)\n",
        "plt.show()\n",
        "\n",
        "# plot predicted vs actual spreads\n",
        "plt.plot(spread_pred_series, label='Predicted Spread')\n",
        "plt.plot(spread_gt_series, label='Actual Spread')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NuBWAsxXTqDV"
      },
      "id": "NuBWAsxXTqDV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trade(\n",
        "    S1: pd.Series,\n",
        "    S2: pd.Series,\n",
        "    spread: pd.Series, # model-predicted spread for the strategy\n",
        "    window_long: int,\n",
        "    window_short: int,\n",
        "    initial_cash: float = 100000,\n",
        "    position_threshold: float = 1.0,\n",
        "    clearing_threshold: float = 0.5,\n",
        "    risk_fraction: float = 0.1\n",
        ") -> list:\n",
        "    ma_long = spread.rolling(window=window_long, center=False).mean()\n",
        "    ma_short = spread.rolling(window=window_short, center=False).mean()\n",
        "    std = spread.rolling(window=window_short, center=False).std()\n",
        "    zscore = (ma_long - ma_short)/std\n",
        "\n",
        "    cash = initial_cash\n",
        "    qty_s1 = 0\n",
        "    qty_s2 = 0\n",
        "    returns = [initial_cash]\n",
        "    position = 0 # 0: neutral, 1: long, -1: short\n",
        "\n",
        "    for i in range(len(spread)):\n",
        "        price_s1 = S1.iloc[i]\n",
        "        price_s2 = S2.iloc[i]\n",
        "        beta = spread.iloc[i]\n",
        "        equity = cash + qty_s1 * price_s1 - qty_s2 * price_s2\n",
        "\n",
        "        # Enter short spread (short S1, long beta S2)\n",
        "        if position == 0 and zscore.iloc[i] > position_threshold:\n",
        "            position = -1\n",
        "            position_size = equity * risk_fraction\n",
        "            qty_s1 = -position_size / price_s1\n",
        "            qty_s2 = (position_size * beta) / price_s2\n",
        "            cash -= (qty_s1 * price_s1 - qty_s2 * price_s2)\n",
        "\n",
        "        # Enter long spread (long S1, short beta S2)\n",
        "        elif position == 0 and zscore.iloc[i] < -position_threshold:\n",
        "            position = 1\n",
        "            position_size = equity * risk_fraction\n",
        "            qty_s1 = position_size / price_s1\n",
        "            qty_s2 = - (position_size * beta) / price_s2\n",
        "            cash -= (qty_s1 * price_s1 - qty_s2 * price_s2)\n",
        "\n",
        "        # Exit to neutral when spread reverts\n",
        "        elif position != 0 and abs(zscore.iloc[i]) < clearing_threshold:\n",
        "            cash += qty_s1 * price_s1 - qty_s2 * price_s2\n",
        "            qty_s1 = 0\n",
        "            qty_s2 = 0\n",
        "            position = 0\n",
        "\n",
        "        equity = cash + qty_s1 * price_s1 - qty_s2 * price_s2\n",
        "        returns.append(equity)\n",
        "\n",
        "    return returns\n",
        "\n",
        "spread_pred_series = pd.Series(y_hat.squeeze(), index=test.index[look_back:])  # align lengths\n",
        "spread_gt_series = pd.Series(y_true.squeeze(), index=test.index[look_back:])\n",
        "returns = trade(\n",
        "    S1 = test['S1_close'].iloc[look_back:],\n",
        "    S2 = test['S2_close'].iloc[look_back:],\n",
        "    spread = spread_pred_series,\n",
        "    window_long = 30,\n",
        "    window_short = 5,\n",
        "    position_threshold = 1.0,\n",
        "    clearing_threshold = 0.5\n",
        ")\n",
        "print(\"Simulated P&L:\", returns[-1])\n",
        "print(f\"Return % over period: {100*((returns[-1] - returns[0]) / returns[0])}%\")\n",
        "print(f\"Return % YoY: {((returns[-1] / returns[0])**(365/len(returns)) - 1) * 100}\")\n",
        "\n",
        "# plot returns\n",
        "plt.plot(returns)\n",
        "plt.show()\n",
        "\n",
        "# plot predicted vs actual spreads\n",
        "plt.plot(spread_pred_series, label='Predicted Spread')\n",
        "plt.plot(spread_gt_series, label='Actual Spread')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lTLL_tZnFz_6"
      },
      "id": "lTLL_tZnFz_6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get a more reliable plot, we will test out a set of different threshold combinations to be able to plot uncertainty / standard deviation in our returns.\n",
        "\n"
      ],
      "metadata": {
        "id": "VvbDGuWkkOEK"
      },
      "id": "VvbDGuWkkOEK"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_with_uncertainty(position_thresholds=None, clearing_thresholds=None,\n",
        "                          long_windows=None, short_windows=None):\n",
        "  threshold_combinations = list(itertools.product(position_thresholds, clearing_thresholds))\n",
        "\n",
        "  if position_thresholds is not None and clearing_thresholds is not None:\n",
        "      threshold_combinations = list(itertools.product(position_thresholds, clearing_thresholds))\n",
        "      param_type = 'thresholds'\n",
        "  elif long_windows is not None and short_windows is not None:\n",
        "      threshold_combinations = list(itertools.product(long_windows, short_windows))\n",
        "      param_type = 'windows'\n",
        "  else:\n",
        "      raise ValueError(\"Must specify either (position_thresholds and clearing_thresholds) or (long_windows and short_windows)\")\n",
        "\n",
        "  all_returns = []\n",
        "\n",
        "  for a, b in threshold_combinations:\n",
        "      if param_type == 'thresholds':\n",
        "          returns = trade(\n",
        "              S1=test['S1_close'].iloc[look_back:],\n",
        "              S2=test['S2_close'].iloc[look_back:],\n",
        "              spread=spread_pred_series,\n",
        "              window_long=30,\n",
        "              window_short=5,\n",
        "              position_threshold=a,\n",
        "              clearing_threshold=b\n",
        "          )\n",
        "          # print(f\"Returns for (pt={a},ct={b}) -> {returns[-1]}\")\n",
        "      else:\n",
        "          returns = trade(\n",
        "              S1=test['S1_close'].iloc[look_back:],\n",
        "              S2=test['S2_close'].iloc[look_back:],\n",
        "              spread=spread_pred_series,\n",
        "              window_long=a,\n",
        "              window_short=b,\n",
        "              position_threshold=0.8,\n",
        "              clearing_threshold=0.2\n",
        "          )\n",
        "          # print(f\"Returns for (wl={a},ws={b}) -> {returns[-1]}\")\n",
        "\n",
        "      all_returns.append(returns)\n",
        "\n",
        "  # turn into numpy\n",
        "  returns_array = np.vstack([np.array(r) for r in all_returns])\n",
        "\n",
        "  # mean and stdev for plotting\n",
        "  mean_returns = returns_array.mean(axis=0)\n",
        "  std_returns = returns_array.std(axis=0)\n",
        "  time_axis_series = test.index[look_back - 1:]\n",
        "\n",
        "  std_dev_pct = (std_returns / mean_returns[0]) * 100\n",
        "\n",
        "\n",
        "  if param_type == \"thresholds\":\n",
        "      print(f\"position threshold ({min(position_thresholds):.2f}-{max(position_thresholds):.2f}), \"\n",
        "                f\"clearing threshold ({min(clearing_thresholds):.2f}-{max(clearing_thresholds):.2f})\")\n",
        "  else:\n",
        "      print(f\"short window ({min(short_windows)}-{max(short_windows)}), \"\n",
        "                f\"long window ({min(long_windows)}-{max(long_windows)})\")\n",
        "  print(f\"Return % over period: {100 * ((mean_returns[-1] - mean_returns[0]) / mean_returns[0]):.2f}% ± {std_dev_pct[-1]:.2f}%\")\n",
        "  print(f\"Return % YoY (mean and std dev): {((mean_returns[-1] / mean_returns[0])**(365 / len(mean_returns)) - 1) * 100:.2f}% ± {((std_returns[-1] / mean_returns[0]) * np.sqrt(365 / len(mean_returns))) * 100:.2f}%\")\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(time_axis_series, mean_returns, label='Mean Strategy Returns')\n",
        "  plt.fill_between(time_axis_series, mean_returns - std_returns, mean_returns + std_returns, alpha=0.3, label='±1 Std Dev')\n",
        "  plt.xlabel('Time')\n",
        "  plt.ylabel('Cumulative Return')\n",
        "  if param_type == \"thresholds\":\n",
        "      plt.title(f\"Trading Strategy Returns - position threshold ({min(position_thresholds):.2f}-{max(position_thresholds):.2f}), \"\n",
        "                f\"clearing threshold ({min(clearing_thresholds):.2f}-{max(clearing_thresholds):.2f})\")\n",
        "  else:\n",
        "      plt.title(f\"Trading Strategy Returns - short window ({min(short_windows)}-{max(short_windows)}), \"\n",
        "                f\"long window ({min(long_windows)}-{max(long_windows)})\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "threshold_ranges = [\n",
        "    (3.0, 3.5, 0.6, 0.7),\n",
        "    (1.0, 2.0, 0.1, 0.3),\n",
        "    (1.5, 2.5, 0.2, 0.4),\n",
        "    (2.0, 3.0, 0.3, 0.5),\n",
        "    (2.5, 3.5, 0.4, 0.6),\n",
        "    (3.0, 4.0, 0.5, 0.7),\n",
        "    (4.0, 5.0, 0.6, 0.8),\n",
        "    (1.0, 3.0, 0.2, 0.6),\n",
        "    (2.0, 4.0, 0.3, 0.7),\n",
        "    (3.0, 5.0, 0.4, 0.9),\n",
        "    (1.5, 4.5, 0.1, 0.5),\n",
        "    (1.2, 2.8, 0.2, 0.4),\n",
        "    (2.3, 3.7, 0.3, 0.6),\n",
        "    (3.1, 4.2, 0.4, 0.7),\n",
        "    (1.8, 3.6, 0.5, 0.8),\n",
        "    (2.5, 4.9, 0.6, 0.9),\n",
        "    (1.0, 5.0, 0.1, 1.0),\n",
        "    (2.0, 2.5, 0.3, 0.4),\n",
        "    (3.0, 3.5, 0.5, 0.6),\n",
        "    (4.0, 4.5, 0.6, 0.7),\n",
        "    (1.5, 3.0, 0.2, 0.3)\n",
        "]\n",
        "\n",
        "\n",
        "for min_position, max_position, min_clearing, max_clearing in threshold_ranges:\n",
        "  position_thresholds = np.linspace(min_position, max_position, num=10)\n",
        "  clearing_thresholds = np.linspace(min_clearing, max_clearing, num=10)\n",
        "  plot_with_uncertainty(position_thresholds, clearing_thresholds)"
      ],
      "metadata": {
        "id": "0KPqG2T0lLAm"
      },
      "id": "0KPqG2T0lLAm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some examples of outputs for understanding the form of the data better:\n",
        "\n",
        "`len(trainX_untr)`\n",
        "```\n",
        "2238\n",
        "```\n",
        "\n",
        "`len(trainX_untr[0])`\n",
        "```\n",
        "34\n",
        "```\n",
        "\n",
        "Context: The 34 features consist of\n",
        "* 10 technical indicators for both S1 and S2 (total 20)\n",
        "* S1_close/open/high/low/volume, same for S2 (total 10)\n",
        "* Pair spreads: close, open, high, low (total 4)\n",
        "\n",
        "\n",
        "`trainX_untr[0] `\n",
        "```\n",
        "array([ 2.76970068e+01,  4.91006247e+01,  2.89730484e+01,  4.91027293e+01,\n",
        "        2.89891358e+01,  4.91343834e+01,  2.60513431e+01,  4.87784465e+01,\n",
        "        5.03546207e+05,  7.43386097e+03,  4.49718063e+01,  5.82671806e+01,\n",
        "        5.75577766e+01,  8.28358144e+01,  7.59406546e+01,  3.92425336e+02,\n",
        "        1.05376719e+05, -9.14930577e+03,  1.56861725e+00,  7.63660812e-01,\n",
        "        2.85638197e+01,  4.83799321e+01,  8.30115861e+00,  2.65305580e+01,\n",
        "        2.84687992e+01,  4.85740999e+01,  2.27306259e-01,  4.38795633e-02,\n",
        "       -1.31033890e+00,  1.00654755e-01, -6.89227038e+00, -5.61453974e+00,\n",
        "       -5.72867758e+00, -8.18838280e+00])\n",
        "```\n",
        "\n",
        "`len(trainY_untr)`\n",
        "```\n",
        "2238\n",
        "```\n",
        "\n",
        "\n",
        "`len(trainY_untr)[0]`\n",
        "```\n",
        "1\n",
        "```\n",
        "\n",
        "`trainY_untr[0]`\n",
        "```\n",
        "array([27.81830352])\n",
        "```\n",
        "\n",
        "`trainY_untr[:20]`\n",
        "```\n",
        "[array([27.81830352]),\n",
        " array([27.42025825]),\n",
        " array([25.9191175]),\n",
        " array([22.98625305]),\n",
        " array([20.5661885]),\n",
        " array([21.23151271]),\n",
        " array([24.11603916]),\n",
        " array([25.605551]),\n",
        " array([26.16966699]),\n",
        " array([26.46204422]),\n",
        " array([25.33065673]),\n",
        " array([25.72835342]),\n",
        " array([25.91998167]),\n",
        " array([25.70591191]),\n",
        " array([25.83366537]),\n",
        " array([26.33152235]),\n",
        " array([26.35160811]),\n",
        " array([26.2352556]),\n",
        " array([26.03820719]),\n",
        " array([25.75521362])]\n",
        " ```\n",
        "\n",
        "`trainX_sliding.shape` (when using look_back=20)\n",
        "\n",
        "```\n",
        " (2219, 20, 34)\n",
        "```\n"
      ],
      "metadata": {
        "id": "JHsrTP01tun3"
      },
      "id": "JHsrTP01tun3"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}