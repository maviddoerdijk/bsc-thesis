{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# if \"preprocessing\" folder in current folders -> cd back to original folder\n",
        "%cd /content\n",
        "import os\n",
        "if os.path.exists(\"bsc-thesis\"):\n",
        "  # if bsc-thesis folder already exists; completely remove\n",
        "  !rm -rf bsc-thesis\n",
        "\n",
        "# cloning repo\n",
        "branch = \"main\"\n",
        "!git clone --branch $branch https://github.com/maviddoerdijk/bsc-thesis.git\n",
        "\n",
        "# moving into project dir\n",
        "%cd bsc-thesis/src\n",
        "%ls"
      ],
      "metadata": {
        "id": "NGxy-8UTQD_6",
        "outputId": "8148a944-ef6f-4be8-85b6-a5b7db5cf97b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "NGxy-8UTQD_6",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'bsc-thesis'...\n",
            "remote: Enumerating objects: 805, done.\u001b[K\n",
            "remote: Counting objects: 100% (185/185), done.\u001b[K\n",
            "remote: Compressing objects: 100% (152/152), done.\u001b[K\n",
            "remote: Total 805 (delta 98), reused 93 (delta 33), pack-reused 620 (from 1)\u001b[K\n",
            "Receiving objects: 100% (805/805), 26.86 MiB | 28.13 MiB/s, done.\n",
            "Resolving deltas: 100% (452/452), done.\n",
            "Filtering content: 100% (32/32), 1.75 GiB | 149.02 MiB/s, done.\n",
            "/content/bsc-thesis/src\n",
            "\u001b[0m\u001b[01;34mbacktesting\u001b[0m/  \u001b[01;34mdata\u001b[0m/      main.ipynb  \u001b[01;34mmodels\u001b[0m/         \u001b[01;34mutils\u001b[0m/\n",
            "\u001b[01;34mconfig\u001b[0m/       \u001b[01;34mexternal\u001b[0m/  main.py     \u001b[01;34mpreprocessing\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "287ffabf",
      "metadata": {
        "id": "287ffabf",
        "outputId": "df519b33-12f7-4691-ac3f-ff10e98936bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ta) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ta) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=a2a3507dddca2cae5a6c14c534ade4b53b5d4365550981f1f6277c671f9815ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/d7/29/7781cc5eb9a3659d032d7d15bdd0f49d07d2b24fec29f44bc4\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n",
            "Collecting pykalman\n",
            "  Downloading pykalman-0.10.1-py2.py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from pykalman) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pykalman) (24.2)\n",
            "Collecting scikit-base<0.13.0 (from pykalman)\n",
            "  Downloading scikit_base-0.12.2-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: scipy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from pykalman) (1.15.3)\n",
            "Downloading pykalman-0.10.1-py2.py3-none-any.whl (248 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.5/248.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_base-0.12.2-py3-none-any.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-base, pykalman\n",
            "Successfully installed pykalman-0.10.1 scikit-base-0.12.2\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from PyWavelets) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Goal: plot test MSE vs profitability\n",
        "!pip install ta\n",
        "!pip install pykalman\n",
        "!pip install PyWavelets\n",
        "\n",
        "FLASH_ATTN = False # set to true if using this\n",
        "if FLASH_ATTN:\n",
        "  !pip install flash-attn==2.6.3 # optional but recommended by the repo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Module imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Optional, Callable, Dict, Any\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm # note: using tqdm.auto usually automatically chooses the right import based on whether you're in CLI, notebook or somewhere else\n",
        "import torch.nn as nn\n",
        "import itertools\n",
        "from pykalman import KalmanFilter\n",
        "import ast\n",
        "import re\n",
        "from tabulate import tabulate\n",
        "from datetime import datetime\n",
        "\n",
        "# Custom Imports\n",
        "from models.statistical_models import create_dataset, default_normalize, rmse_metric, acc_metric, kalman_filter_average, kalman_filter_regression, kalman_filter_regression_multivariate\n",
        "from preprocessing.cointegration import find_cointegrated_pairs\n",
        "from preprocessing.data_preprocessing import filter_pairs_data\n",
        "from preprocessing.technical_indicators import combine_pairs_data\n",
        "from preprocessing.wavelet_denoising import wav_den\n",
        "from preprocessing.filters import step_1_filter_remove_nans, step_2_filter_liquidity\n",
        "from backtesting.trading_strategy import trade, get_gt_yoy_returns_test_dev\n",
        "from backtesting.utils import calculate_return_uncertainty\n",
        "from utils.visualization import plot_return_uncertainty, plot_comparison\n",
        "from utils.helpers import _get_train_dev_frac\n",
        "\n",
        "## workflow imports\n",
        "from models.statistical_models import execute_kalman_workflow\n",
        "from models.transformer_model import execute_transformer_workflow\n",
        "# from models.time_moe_model import execute_timemoe_workflow\n",
        "\n",
        "## specific caching imports (should be changed in case you want to gather data live)\n",
        "from data.scraper import load_cached_etf_tickers\n",
        "from data.data_collection_cache import gather_data_cached, _get_filename, gather_pairs_data_cached, gather_data_cached_using_truncate\n",
        "\n",
        "# Any other changes to be made throughout the entire notebook\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "inspect_func = False\n",
        "if inspect_func:\n",
        "  import inspect\n",
        "  print(inspect.getsource(trade)) # in this case, check whether tqdm was actually added"
      ],
      "metadata": {
        "id": "sk2OjZ1PQURz"
      },
      "id": "sk2OjZ1PQURz",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Goal: plot test MSE vs profitability\n",
        "\n",
        "### DATA GATHER ###\n",
        "start_year = 2008\n",
        "end_year = 2024\n",
        "startDateStr = f'{start_year}-01-01'\n",
        "endDateStr = f'{end_year}-12-31'\n",
        "startDateStrTest = f'{end_year}-01-01'\n",
        "endDateStrTest = f'{end_year}-12-31'\n",
        "train_frac, dev_frac = _get_train_dev_frac(startDateStr, endDateStr, startDateStrTest, endDateStrTest)\n",
        "\n",
        "instrumentIdsNASDAQandNYSE = load_cached_etf_tickers()\n",
        "data = gather_data_cached_using_truncate(startDateStr, endDateStr, instrumentIdsNASDAQandNYSE, cache_dir='../src/data/cache')\n",
        "data_close_filtered_1, data_open_filtered_1, data_high_filtered_1, data_low_filtered_1, data_vol_filtered_1, data_original_format_filtered_1 = step_1_filter_remove_nans(data['close'], data['open'], data['high'], data['low'], data['vol'], data)\n",
        "data_close_filtered_2, data_open_filtered_2, data_high_filtered_2, data_low_filtered_2, data_vol_filtered_2, data_original_format_filtered_2 = step_2_filter_liquidity(data_close_filtered_1, data_open_filtered_1, data_high_filtered_1, data_low_filtered_1, data_vol_filtered_1, data_original_format_filtered_1)\n",
        "\n",
        "pairs_data_filtered = gather_pairs_data_cached(startDateStr, endDateStr, instrumentIdsNASDAQandNYSE, cache_dir='../src/data/cache')\n",
        "if not pairs_data_filtered:\n",
        "  scores, pvalues, pairs = find_cointegrated_pairs(data_original_format_filtered_2)\n",
        "  pairs_data = {key:value[1]  for (key, value) in pairs.items()}\n",
        "  pairs_data = sorted(pairs_data.items(), key=lambda x: x[1])\n",
        "  pairs_data_filtered = filter_pairs_data(pairs_data) # filter based on cointegration in such a way that we can simply pick the highest pair of stocks in the list.\n",
        "\n",
        "\n",
        "### DATA GATHER ###\n",
        "\n",
        "### EXTRA VARS ###\n",
        "\n",
        "verbose = False\n",
        "\n",
        "### EXTRA VARS ###"
      ],
      "metadata": {
        "id": "rQCKpH6dQsGT"
      },
      "id": "rQCKpH6dQsGT",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Definition of trade functions ###\n",
        "\n",
        "\n",
        "### Definition of trade functions ###"
      ],
      "metadata": {
        "id": "RNrA7te1Uroz"
      },
      "id": "RNrA7te1Uroz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Gathering of results ###\n",
        "\n",
        "\n",
        "# Extract the most highly cointegrated pairs\n",
        "for i in range(len(pairs_data_filtered[:5])):\n",
        "  ticker_a, ticker_b = pairs_data_filtered[i][0][0], pairs_data_filtered[i][0][1]\n",
        "  pairs_timeseries_df = combine_pairs_data(data_close_filtered_2, data_open_filtered_2, data_high_filtered_2, data_low_filtered_2, data_vol_filtered_2, ticker_a, ticker_b)\n",
        "\n",
        "  burn_in = 30\n",
        "\n",
        "  pairs_timeseries_df_burned_in = pairs_timeseries_df.iloc[burn_in:].copy()\n",
        "\n",
        "  total_len = len(pairs_timeseries_df_burned_in)\n",
        "  train_size = int(total_len * train_frac)\n",
        "  dev_size   = int(total_len * dev_frac)\n",
        "  test_size  = total_len - train_size - dev_size # not used, but for clarity\n",
        "\n",
        "  train = pairs_timeseries_df_burned_in.iloc[:train_size]\n",
        "  dev   = pairs_timeseries_df_burned_in.iloc[train_size:train_size + dev_size]\n",
        "  test  = pairs_timeseries_df_burned_in.iloc[train_size + dev_size:]\n",
        "\n",
        "  ## Getting timeseries ##\n",
        "  # 0. ground truth\n",
        "  pairs_timeseries_df_spread_close_test = test['Spread_Close']\n",
        "\n",
        "  # 1. artificial noise\n",
        "  std_dev = 0.5\n",
        "  noise = np.random.normal(0, std_dev, size=len(test))\n",
        "  pairs_timeseries_df_spread_close_test_added_noise = test['Spread_Close'] + noise\n",
        "\n",
        "  # 2. kalman predictions\n",
        "\n",
        "  # 3. Transformer predictions\n",
        "\n",
        "  # 4. Time-MoE predictions\n",
        "\n",
        "  ## Getting timeseries ##\n",
        "\n",
        "  print(f\"Profits ground truth: \")\n",
        "  print(f\"Profits GT + artificial noise (0.5 std dev):\")\n",
        "  print(f\"Profits Kalman Predictions (TA): \")\n",
        "  print(f\"Profits Kalman Predictions: (no TA)\")\n",
        "  print(f\"Profits Transformer Predictions (TA): \")\n",
        "  print(f\"Profits Transformer Predictions (no TA): \")\n",
        "  print(f\"Profits TimeMoE Predictions: (no TA)\")\n",
        "\n",
        "### Gathering of results ###"
      ],
      "metadata": {
        "id": "wKpW7b1_RITj",
        "outputId": "5a2d4764-cc40-4dfe-eb8c-5e29ffc2da96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        }
      },
      "id": "wKpW7b1_RITj",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GT YOY: -0.14205346835406496\n",
            "Yoy Returns: 111.47630065356755 +- 4.151886878592863\n",
            "TEST MSE: 4.550297884171458\n",
            "GT YOY: -0.14205346835406496\n",
            "Yoy Returns: 22.647256732075952 +- 1.842858730199155\n",
            "TEST MSE: 9.174108958976943\n",
            "GT YOY: -0.14205346835406507\n",
            "Yoy Returns: 0.30034669394934466 +- 1.0174518594979556\n",
            "TEST MSE: 4.04371371905866\n",
            "GT YOY: -0.14205346835406507\n",
            "Yoy Returns: -0.14205346835406463 +- 1.7290725485624172e-16\n",
            "TEST MSE: 2.568115548357022\n",
            "GT YOY: -0.14205346835406496\n",
            "Yoy Returns: 5.801870150956745 +- 6.8502763685531916\n",
            "TEST MSE: 0.08738206185959528\n",
            "GT YOY: -0.14205346835406496\n",
            "Yoy Returns: 263.47623474934926 +- 8.7237585852514\n",
            "TEST MSE: 8.385346618107237\n",
            "GT YOY: -0.14205346835406496\n",
            "Yoy Returns: 17.82638268073145 +- 1.457368089849135\n",
            "TEST MSE: 7.933472195539503\n",
            "GT YOY: 12.7514979444988\n",
            "Yoy Returns: 1.6100151302435912 +- 1.5399841657690125\n",
            "TEST MSE: 3.0309693268915456\n",
            "GT YOY: 8.202220572424721\n",
            "Yoy Returns: -0.08113264050853308 +- 0.1173835460619129\n",
            "TEST MSE: 2.169843550245091\n",
            "GT YOY: -0.14205346835406496\n",
            "Yoy Returns: 12.215229306878552 +- 9.22856832949247\n",
            "TEST MSE: 0.22611978236928637\n",
            "GT YOY: 0.5321929691529574\n",
            "Yoy Returns: 126.60693445089382 +- 8.03355178178684\n",
            "TEST MSE: 10.422077130274412\n",
            "GT YOY: -0.14205346835406496\n",
            "Yoy Returns: 16.464257374800063 +- 9.26519678724547\n",
            "TEST MSE: 0.061823137834676224\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-123cfbb7476f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mpairs_timeseries_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_pairs_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_close_filtered_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_open_filtered_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_high_filtered_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_low_filtered_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_vol_filtered_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecute_kalman_workflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpairs_timeseries_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_tup_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"({ticker_a},{ticker_b})\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_frac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_frac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_frac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_frac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mgt_yoy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myoy_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myoy_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gt_yoy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_mse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'yoy_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'yoy_std'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"GT YOY: {gt_yoy}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/bsc-thesis/src/models/statistical_models.py\u001b[0m in \u001b[0;36mexecute_kalman_workflow\u001b[0;34m(pair_data, col_s1, col_s2, burn_in, train_frac, dev_frac, seed, look_back, denoise_fn, scaler_factory, scaler_kwargs, normalise_fn, delta, obs_cov_reg, trans_cov_avg, obs_cov_avg, return_datasets, verbose, add_technical_indicators, result_parent_dir, filename_base, pair_tup_str)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m       \u001b[0;31m# recursive least squares multivariate regression, using the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m       \u001b[0mbeta_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkalman_filter_regression_multivariate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m       \u001b[0mforecast_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbeta_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0mforecast_dev\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbeta_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/bsc-thesis/src/models/statistical_models.py\u001b[0m in \u001b[0;36mkalman_filter_regression_multivariate\u001b[0;34m(X, y, delta, obs_cov)\u001b[0m\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mstate_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate_means\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pykalman/standard.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1221\u001b[0m         ) = self._initialize_parameters()\n\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         (_, _, _, filtered_state_means, filtered_state_covariances) = _filter(\n\u001b[0m\u001b[1;32m   1224\u001b[0m             \u001b[0mtransition_matrices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m             \u001b[0mobservation_matrices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pykalman/standard.py\u001b[0m in \u001b[0;36m_filter\u001b[0;34m(transition_matrices, observation_matrices, transition_covariance, observation_covariance, transition_offsets, observation_offsets, initial_state_mean, initial_state_covariance, observations)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mfiltered_state_means\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mfiltered_state_covariances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_filter_correct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0mobservation_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0mobservation_covariance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pykalman/standard.py\u001b[0m in \u001b[0;36m_filter_correct\u001b[0;34m(observation_matrix, observation_covariance, observation_offset, predicted_state_mean, predicted_state_covariance, observation)\u001b[0m\n\u001b[1;32m    284\u001b[0m         )\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         kalman_gain = np.dot(\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mpredicted_state_covariance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpinv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_observation_covariance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sHzpr614SE3l"
      },
      "id": "sHzpr614SE3l",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}